{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hopfield Model and data reconstuction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors:\n",
    "- **Ballarin Marco**,   $\\textit{1228022, Physics of Data}$\n",
    "- **Iriarte Delfina**,  $\\textit{1231682, Physics of Data}$\n",
    "- **Segalini Beatrice**, $\\textit{1234430, Physics of Data}$\n",
    "- **Magenya Joemah**,   $\\textit{1233392, Physics of Data}$\n",
    "\n",
    "### Supervised by:\n",
    "- Professor Baiesi Marco, marco.baiesi@unipd.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9AzwsBa14XR"
   },
   "source": [
    "Artificial Neural Networks (ANN) are computational techniques that aim to realize a very simplified model of the human brain. In this way, ANN try to learn tasks mimicking the behavior of brain, which is composed of a large set of elements, specialized cells called neurons. Each single neuron is a very simple entity but the power of the brain is given by the fact that neurons are numerous and strongly interconnected between them. The human brain is one of the most computationally efficient device that is why in the late years a lot of effort has been done in order to develop an artificial version of it, as a matter of fact companies like Telsa motors are developing self driving cars which are based on ANN to implement the behavior of the human brain in computer systems. \n",
    "\n",
    "The easiest way to represent a neuron is either \"on or off\" with the synapsys either eccitative or inibitive. We can easily map the neural network into a spin system by mapping neurons in spins $\\{s_i\\}_{i=1,\\cdots,N}$ and synapsys into the magnetic coupling $J_{ij}$.\n",
    "By a specific representation of these coupling costant we can define the Hopfield model.\n",
    "\n",
    "## The Hopfield Model\n",
    "\n",
    "The Hopfield model is a fully connected neural network able to recall stored memories starting from a noisy or distorted input. The Hopfield network consists of $N$ neurons connected through symmetric bidirectional links. The neuron interactions are encoded in the connection matrix, a $N × N$ real symmetric matrix without self-interaction terms, whose $J_{ij}$ entries define the weight of the connection between neuron $i$ and $j$. \n",
    "\n",
    "The model respect the following mapping:\n",
    "\n",
    "$$\n",
    "n = \\{0,1\\} \\longrightarrow s = \\{-1,1\\}    \n",
    "$$\n",
    "                                   \n",
    "where $n$ is the neuron and $s$ the spin. \n",
    "The correct transformation of this mapping is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "s = f(n) = \\frac{2n-1}{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We define the synapses as:\n",
    "$$\n",
    "J_{ij}=\\begin{cases}\n",
    "+1 & \\mbox{ excitatory synapses} \\\\\n",
    "-1 & \\mbox{ inhibitory synapses}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A neuron is activated if it receives a sufficient number of active impulses, and we can compute these impulses as follows:\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_i(t) = \\sum^N_{j=1,j\\neq i} J_{ij}(s_j(t)+1)\n",
    "\\label{eq:h} \n",
    "\\end{equation}\n",
    "$$\n",
    "It is important to notice that $j\\neq i$ since the neuron $\\textit{does not}$ interacts with itself.\n",
    "To decide if these impulses are sufficient to activate the neuron, we apply a non-linear function to the impulses:\n",
    "$$\n",
    "\\begin{equation}\n",
    "s_i(t+1)=sign{\\left(h_i(t)-\\theta_i\\right)}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\theta_i$ is a fixed threshold. We choose this threshold in a way that is useful for the calculations:\n",
    "$$\n",
    "\\theta_i = \\sum^N_{j=1,j\\neq i} J_{ij}.\n",
    "$$\n",
    "By applying this threshold, we obtain the following $\\textbf{update rule}$, which we will be using below in the code:\n",
    "$$\n",
    "\\\\\n",
    "\\begin{equation}\n",
    "s_i(t+1)=sign{\\left(\\sum^N_{j=1,j\\neq i} J_{ij}s_j(t)\\right)}\n",
    "\\label{eq:upd}\n",
    "\\end{equation}\n",
    "\\\\\n",
    "$$\n",
    "The $\\textbf{Hopfield model}$ that we will exploit in our work consists of a specific choice for the synapses ($\\textit{magnetic couplings}$):\n",
    "$$\n",
    "\\\\\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "J_{ii}=0 & \\mbox{ known as Hebb rule} \\\\\n",
    "J_{ij}=\\frac{1}{N}\\sum_{\\mu=1}^{p} \\xi^\\mu_i\\xi^\\mu_j\n",
    "\\end{cases}\n",
    "\\label{eq:hop}\n",
    "\\end{equation}\n",
    "\\\\\n",
    "$$\n",
    "where the $\\vec{\\xi^\\mu}$ are $p<<N$ excitatory pattern with $\\xi^\\mu_i=\\{+1,-1\\}$.\n",
    "This choice encodes these patterns in the couplings and gives to the system some interesting properties:\n",
    "- If the system starts from a configuration equal to a pattern $\\vec{\\xi^\\mu}$ and we apply the update rule of Eq $\\eqref{eq:upd}$, it remains in that pattern $\\forall t$.\n",
    "- By solving the system from a statistical mechanics point of view, it turns out that all the minima of the system corresponds to the patterns.\n",
    "\n",
    "It is instructive to see the proof of the stability of the patterns.\n",
    "$$\n",
    "s(1)=sign{\\left(\\sum^N_{j=1} J_{ij}s_j(0)\\right)}\\overset{s_j(0)=\\xi^\\mu_j}{=}\n",
    "sign{\\left(\\sum^N_{j=1} \\frac{1}{N}\\sum^p_{\\nu=1}\\xi_i^\\nu\\xi_j^\\nu\\xi_j^\\mu\\right)} \n",
    "\\\\=sign{\\left(\\sum^N_{\\nu=1}\\xi_i^\\nu\\frac{1}{N}\\sum^p_{j=1}\\xi_j^\\nu\\xi_j^\\mu \\right)} \n",
    "=sign{\\left(\\sum^N_{\\nu=1}\\xi_i^\\nu(\\delta_{\\mu\\nu}+O(N^{-\\frac{1}{2}}) \\right)}\n",
    "\\simeq sign\\left(\\xi^\\mu_i \\right)=\\xi^\\mu_i\n",
    "$$\n",
    "And so the property enuciated before holds.\n",
    "\n",
    "From now on we will talk about spins and not neurons anymore.\n",
    "\n",
    "This model take into account fully connected systems, where each spin is connected to all the others. But what happen if we consider an interaction lenght $R$? It is an interesting question to look at how the results vary with this assumption.\n",
    "\n",
    "Moreover, this type of system does not need training, differently to other neural networks, and so it can be really interesting if the results are good.\n",
    "\n",
    "In conclusion, the aim of this work is to start from this theoretical model and further analyze it, by studying the stability of the patterns, the recovering of corrupted ones and simulate its dynamics with a Montecarlo method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp\n",
    "from copy import deepcopy\n",
    "import time as time\n",
    "import pylab as pb \n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section, we are going to discuss about the stability of the Hopfield model, in both MF and NMF, especially focusing on these main points:\n",
    "\n",
    "- study of the dependence of error of the algorithm with respect to number of patterns; \n",
    "- time scaling with size of the system;\n",
    "- time scaling with the number of patterns;\n",
    "- dependence of the error on the number of neighbors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "L = 10   # Length of the edge of the image\n",
    "N = L**2 # Number of spins \n",
    "p = 10   # Number of patterns\n",
    "MF = 0   # Use or not the Mean Field strategy: if MF = 1 uses MF, if MF = 0 only spins at\n",
    "         # a distance R interacts\n",
    "R = 3    # The 3-nearest neighbor interacts\n",
    "np.random.seed(1234) # Seed to make the random process reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns\n",
    "def random_patterns(N,p):\n",
    "    xi = np.random.randint(0,2,N*p).reshape(p,N) # Each line is a pattern\n",
    "    xi[xi==0]=-1\n",
    "    return xi\n",
    "\n",
    "xi = random_patterns(N,p)\n",
    "idx = np.random.randint(0,p)\n",
    "plt.imshow(xi[idx].reshape(L,L),cmap='Greys') # This is an example of pattern \n",
    "# -1 = white, +1 = black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coupling constant\n",
    "# Row i is the connections of the i-th neuron with the others.\n",
    "# Note that the Hopfield model requires Jii=0\n",
    "# Note that J is a symmetrical matrix (J=J.T)\n",
    "\n",
    "def MF_coupling(xi,N):\n",
    "    J = 1/N * np.array( [ (xi[:,i]*xi[:,j]).sum() for i in range(N) for j in range(N) ] )\n",
    "    J = J.reshape(N,N)        \n",
    "    for i in range(len(J)):\n",
    "        J[i,i] = 0\n",
    "    return J\n",
    "\n",
    "def R_coupling(xi,N,R):\n",
    "    J = MF_coupling(xi,N)\n",
    "    L = int(np.sqrt(N))\n",
    "    for i in range( J.shape[0] ):\n",
    "        J_temp = J[i].reshape(L,L)\n",
    "        for j in range(L):\n",
    "            y = (i%L -j)**2 \n",
    "            for k in range(L):\n",
    "                if np.sqrt( (i//L - k)**2 + y ) > R: J_temp[j,k] = 0\n",
    "        J[i] = J_temp.reshape(1,N)\n",
    "    return J\n",
    "\n",
    "if MF: J = MF_coupling(xi,N)\n",
    "else: J = R_coupling(xi,N,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update rule\n",
    "\n",
    "def update(sys1):\n",
    "    sys = deepcopy(sys1)\n",
    "    N = len(sys)\n",
    "    temp = np.zeros(N) \n",
    "    for i in range(N):\n",
    "        for j in range(N): \n",
    "            temp[i] +=  J[i][j]*sys[j]\n",
    "        if temp[i] == 0: temp[i] = 2*np.random.randint(0,2)-1\n",
    "    sys = np.sign(temp)\n",
    "    return np.sign(sys)\n",
    "\n",
    "def deterministic_hopfield(sys, t): #t is the number of iterations\n",
    "    for i in range(t):\n",
    "        sys = update(sys)\n",
    "    return sys\n",
    "\n",
    "#we define the error as the number of different pixels between a pattern and the image \n",
    "#normalized to the system size\n",
    "def error_im(xi_idx, sys): #xi_idx is the \"correct\" pattern\n",
    "    wrong_pixels = (np.abs( sys-xi_idx )/2).sum()\n",
    "    return wrong_pixels/len(sys)\n",
    "\n",
    "def assign_pattern(xi,sys): #xi are the patterns\n",
    "    errors = [ error_im(i,sys) for i in xi ]\n",
    "    correct = np.argmin(errors)\n",
    "    return correct \n",
    "\n",
    "#calculate score and error for all the possible patterns\n",
    "def total_error(xi,t):\n",
    "    errors = []\n",
    "    prediction = []\n",
    "    for mu in range(len(xi)):\n",
    "        sys = deterministic_hopfield(xi[mu],t)\n",
    "        errors.append( error_im(xi[mu],sys) )\n",
    "        if assign_pattern(xi,sys)==mu:\n",
    "            prediction.append( 1 )\n",
    "        else: prediction.append( 0 )\n",
    "    errors = (np.array(errors)).mean()\n",
    "    prediction = np.array(prediction).sum()/len(xi)\n",
    "    return errors, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual algorithm\n",
    "\n",
    "idx = np.random.randint(0,p) #randomly chosen pattern\n",
    "sys = deepcopy(xi[idx])\n",
    "\n",
    "sys = deterministic_hopfield(sys,100)\n",
    "wrong_pixels = error_im(xi[idx],sys)\n",
    "assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,6))\n",
    "ax[0].set_title('Pattern')\n",
    "ax[0].imshow(xi[idx].reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Finishing configuration')\n",
    "ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "At first we studied how the performances of the algorithm depended on the number of patterns $p$. The system size was fixed to $L=16$ for the first case, then we changed it to other sizes in order to compare the error and score. Both the Mean Field model (MF) and the Non Mean Field one (NMF) have been studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 16\n",
    "N = L**2\n",
    "\n",
    "P = np.arange(5,N/2,20)\n",
    "scores_16 = [[],[]]\n",
    "errors_16 = [[],[]]\n",
    "\n",
    "for MF in range(2):\n",
    "    for p in P: \n",
    "        p = int(p)\n",
    "        xi = random_patterns(N,p)\n",
    "        if MF: J = MF_coupling(xi,N)\n",
    "        else: J = R_coupling(xi,N,R)\n",
    "        err, pred = total_error(xi,30)    \n",
    "        scores_16[MF].append( pred )\n",
    "        errors_16[MF].append(err)\n",
    "\n",
    "figp, axp = plt.subplots(1,2, figsize=(14,6))\n",
    "axp[0].set_title('Dependance of the error by number of pattern p')\n",
    "axp[0].set_xlabel('Number of pattern p')\n",
    "axp[0].set_ylabel('Average error along the p patterns')\n",
    "axp[0].plot(P,errors_16[0],  '--bo', label='Error, no MF')\n",
    "axp[0].plot(P,errors_16[1], '--go', label='Error, MF')\n",
    "axp[0].legend()\n",
    "\n",
    "axp[1].set_title('Dependance of the prediction of the correct pattern by number of pattern p')\n",
    "axp[1].set_xlabel('Number of pattern p')\n",
    "axp[1].set_ylabel('Fraction of pattern correctly predicted')\n",
    "axp[1].plot(P,scores_16[0], '--bo', label='Score, no MF')\n",
    "axp[1].plot(P,scores_16[1], '--go', label='Score, MF')\n",
    "axp[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\stability_p_error_and_scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependance of error with patterns generally follows an increasing tendency at the beginning of the curve, which then flatters, reaching a plateau. The growth is particularly steep in the case of the NMF model, which reaches its maximum within the first $40$ patterns. It is instead less pronounced for the MF theory: here the highest value ($\\approx 0.25$) is reached only after about $70$ patterns. The plateau of the NMF graph is, on the contrary, more stable, showing almost no oscillation around the value of $0.15$, which is also lower than the MF one.\n",
    "\n",
    "Concerning the dependence of the prediction of the correct pattern with number of patterns, it can be observed that, in case of NMF model, the score is always constant whichever the pattern, thus the independence of the error with number of patterns and the perfect effectiveness of the predictions. However, with the MF model, this stability is not preserved: in fact, after passing the first few patterns, the score drops to about $70\\%$, and oscillates around that value.\n",
    "\n",
    "These results can be justified by the structure of the models themselves: indeed, the Mean Field one is more influenced by similar patterns, since it averages over all the spins, while the Non Mean Field is more \"details oriented\" and will perceive small differences without looking at the whole image. \n",
    "\n",
    "It can be stated, after these considerations, that the Non Mean Field algorithm is more effective in studying the dependance of the error and score as functions of the number of patterns. As a consequence, for further analysis this model will be used.\n",
    "\n",
    "By observing both graphs, it can be observed that there exist an optimal value of $p$ patterns which optimize the performances of the algorithm, which in this case is around $20$ (about $8\\%$ of the system size, which is $\\approx \\sqrt{N} = L$ ). In order to verify if this tendency is preserved, different system sizes are studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "N = L**2\n",
    "P_10 = np.arange(5,N/2,5)\n",
    "scores_10 = []\n",
    "errors_10 = []\n",
    "sigma_10 = []\n",
    "\n",
    "MF = 0\n",
    "\n",
    "for pi in P_10: \n",
    "    err = []\n",
    "    pred = []\n",
    "    for n in range(20):\n",
    "        p = int(pi)\n",
    "        xi = random_patterns(N,p)\n",
    "        if MF: J = MF_coupling(xi,N)\n",
    "        else: J = R_coupling(xi,N,R)\n",
    "        e, p = total_error(xi,30)    \n",
    "        err.append(e)\n",
    "        pred.append(p)\n",
    "    pred = np.array(pred)\n",
    "    err = np.array(err)\n",
    "    sigma_10.append( pred.std() )\n",
    "    scores_10.append( pred.mean() )\n",
    "    errors_10.append( err.mean() )\n",
    "    \n",
    "L = 6\n",
    "N = L**2\n",
    "P_6 = np.arange(2,N/2,1)\n",
    "scores_6 = []\n",
    "errors_6 = []\n",
    "sigma_6 = []\n",
    "\n",
    "for pi in P_6: \n",
    "    err = []\n",
    "    pred = []\n",
    "    for n in range(50):\n",
    "        p = int(pi)\n",
    "        xi = random_patterns(N,p)\n",
    "        if MF: J = MF_coupling(xi,N)\n",
    "        else: J = R_coupling(xi,N,R)\n",
    "        e, p = total_error(xi,30)    \n",
    "        err.append(e)\n",
    "        pred.append(p)\n",
    "    pred = np.array(pred)\n",
    "    err = np.array(err)\n",
    "    sigma_6.append( pred.std() )\n",
    "    scores_6.append( pred.mean() )\n",
    "    errors_6.append( err.mean() )\n",
    "\n",
    "L = 8\n",
    "N = L**2\n",
    "P_8 = np.arange(2,N/2,2)\n",
    "scores_8 = []\n",
    "errors_8 = []\n",
    "sigma_8 = []\n",
    "\n",
    "for pi in P_8: \n",
    "    err = []\n",
    "    pred = []\n",
    "    for n in range(40):\n",
    "        p = int(pi)\n",
    "        xi = random_patterns(N,p)\n",
    "        if MF: J = MF_coupling(xi,N)\n",
    "        else: J = R_coupling(xi,N,R)\n",
    "        e, p = total_error(xi,30)    \n",
    "        err.append(e)\n",
    "        pred.append(p)\n",
    "    pred = np.array(pred)\n",
    "    err = np.array(err)\n",
    "    sigma_8.append( pred.std() )\n",
    "    scores_8.append( pred.mean() )\n",
    "    errors_8.append( err.mean() )\n",
    "\n",
    "figp, axp = plt.subplots(1,2, figsize=(14,6))\n",
    "axp[0].set_title('Dependance of the error by number of pattern p')\n",
    "axp[0].set_xlabel('Number of pattern p')\n",
    "axp[0].set_ylabel('Average error along the p patterns')\n",
    "axp[0].plot(P_6,errors_6, '--co', label='Error, no MF, L = 6')\n",
    "axp[0].plot(P_8,errors_8, '--ro', label='Error, no MF, L = 8')\n",
    "axp[0].plot(P_10,errors_10, '--ko', label='Error, no MF, L = 10')\n",
    "axp[0].legend() \n",
    "\n",
    "axp[1].set_title('Dependance of the prediction of the correct pattern by number of pattern p')\n",
    "axp[1].set_xlabel('Number of pattern p')\n",
    "axp[1].set_ylabel('Fraction of pattern correctly predicted')\n",
    "axp[1].plot(P_6, scores_6, '--co', label='Score, no MF, L = 6')\n",
    "axp[1].plot(P_8, scores_8, '--ro', label='Score, no MF, L = 8')\n",
    "axp[1].plot(P_10, scores_10, '--ko', label='Score, no MF, L = 10')\n",
    "axp[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\err_score_p_diffsizes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the comparison of system with different sizes, one can deduce that the hypothesized tendency is respected. In fact, the scores show the steepest descent with a number of $p$ close to the $\\sqrt{N}$; in the other graph, at the same abscissa, the error raise is the biggest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the time scales with the number of spins N?\n",
    "\n",
    "In this section, the focus of the analysis is moved to implementation time. It is important to state that the actual values derived are not significant, as they strongly depend on the hardware used. We studied different $L$s for both Mean Field and Non Mean Field models, even if we expect the same trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.arange(10,28,4)\n",
    "N = L**2 \n",
    "p = 10\n",
    "times = [[],[]]\n",
    "\n",
    "for MF in range(2):\n",
    "    for n in N:\n",
    "        xi = np.random.randint(0,2,n*p).reshape(p,n)\n",
    "        xi[xi==0]=-1\n",
    "        J = 1/n * np.array( [ (xi[:,i]*xi[:,j]).sum() for i in range(n) for j in range(n) ] )\n",
    "        J = J.reshape(n,n)\n",
    "        for k in range(len(J)):\n",
    "            J[k,k] = 0\n",
    "        idx = np.random.randint(0,p)\n",
    "        start = time.time()\n",
    "        deterministic_hopfield(xi[idx],30)\n",
    "        end = time.time()\n",
    "        times[MF].append(end-start)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "ax.plot(N,times[0], '--bo', label = 'Data with 10 patterns, no MF')\n",
    "ax.plot(N,times[1], '--go', label = 'Data with 10 patterns, MF')\n",
    "ax.set(xlabel = ' Number of spins N', ylabel =' Implementation time [s]', \n",
    "              title = '  Dependency of the time implementation on the size of the system')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two curves above show both an exponential growth of the implementation time as a function of the size of the system. The tendency does not change for both Mean Field and Non Mean Field models: this is consistent with the fact that the two algorithms are implemented with the same logic, just by changing the coupling rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the time scales with the number of pattern p with a fixed N?\n",
    "\n",
    "Now we analyse the behaviour of implementation time with respect to the number of pattern p with a fixed system size, for both Mean Field and Non Mean Field models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 16\n",
    "N = L**2 \n",
    "P = np.arange(10,N,16)\n",
    "times = [[],[]]\n",
    "\n",
    "for MF in range(2):\n",
    "    for p in P:\n",
    "        p = int(p)\n",
    "        xi = np.random.randint(0,2,N*p).reshape(p,N)\n",
    "        xi[xi==0]=-1\n",
    "        J = 1/N * np.array( [ (xi[:,i]*xi[:,j]).sum() for i in range(N) for j in range(N) ] )\n",
    "        J = J.reshape(N,N)\n",
    "        for k in range(len(J)):\n",
    "            J[k,k] = 0\n",
    "        idx = np.random.randint(0,p)\n",
    "        start = time.time()\n",
    "        deterministic_hopfield(xi[idx],30)\n",
    "        end = time.time()\n",
    "        times[MF].append(end-start)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "ax.plot(P,times[0], '--bo', label = 'Data with 16x16 spins, no MF')\n",
    "ax.plot(P,times[1], '--go', label = 'Data with 16x16 spins, MF')\n",
    "ax.set(xlabel = 'Number of patterns p', ylabel =' Implementation time [s]', \n",
    "              title = ' Dependency of the time implementation on the number of patterns')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\stability_pattern_time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph the implementation time as a function of the number of patterns is displayed. The shape of the curve does not outline any particular tendency, and oscillates randomly showing instability.\n",
    "The Mean Field model has less pronounced fluctuations with respect to the Non Mean Field one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the number of neighbors\n",
    "\n",
    "We have seen that the Non Mean Field model seems to be more stable and lead to smaller errors. It is then relevant to study how the model evolves if we change the number $R$ of neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF = 0\n",
    "L = 10\n",
    "N = L**2 \n",
    "P = 10\n",
    "error_R = [[],[]]\n",
    "sigmas_R = [[],[]]\n",
    "R = np.arange(3,L)\n",
    "\n",
    "for MF in range(2):\n",
    "    for r in R: \n",
    "        supp = []\n",
    "        for n in range(50):\n",
    "            xi = random_patterns(N,P)\n",
    "            if MF: J = MF_coupling(xi,N)\n",
    "            else: J = R_coupling(xi,N,r)\n",
    "            err, pred = total_error(xi,30)    \n",
    "            supp.append(err)\n",
    "        supp = np.array(supp)\n",
    "        error_R[MF].append(supp.mean())\n",
    "        sigmas_R[MF].append(supp.std())\n",
    "        \n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "ax.plot(R,error_R[0], '--bo', label = 'no MF')\n",
    "ax.errorbar(R,error_R[0], yerr=sigmas_R[0], fmt='none', color='b')\n",
    "ax.plot(R,error_R[1], '--go', label = 'MF')\n",
    "ax.errorbar(R,error_R[1], yerr=sigmas_R[1], fmt='none', color='g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\R_p.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decreasing tendency in the value of the error can be outlined in the graph above. \n",
    "The errorbars are added after calculating the standard deviation of the error along several iterations, and are significant for the NMF model. Concerning the MF one, we can observe very small errors due to the fact that all the value are indeed close to zero: this happens because we are considering a deterministic evolution and because the MF model does not depend on the number of neighbors, as it is considering an average of all the interactions along the grid.\n",
    "\n",
    "Nevertheless, the most striking feature is that the NMF model approaches the MF one with the increasing number of neighbors. This is just as it can be predicted: as a matter of fact, increasing $R$ means increasing the radius of interactions between spins. By including more and more and more spins, the coupling function will become closer to the average field along all the grid. Hence, the two models will become more similar with bigger $R$s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5cDCWZd1XUi"
   },
   "source": [
    "# Corruption of patterns\n",
    "\n",
    "## Why noise in pattern recognition\n",
    "\n",
    "Noise is the amount of meaningless information contained in corrupted data. It incorporates any type of data that a user system cannot understand or interpret correctly.\n",
    "Using improper procedures to getting rid of noise in the data can lead to inaccurate results and false conlusions. Noise reduction is very important in order to obtain accurate results. \n",
    "\n",
    "We implemented different types of noises to discuss how the patterns differs when they are applied. In this work, we used the following noises:\n",
    "- uniform noise \n",
    "- gaussian noise\n",
    "- cauchy noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReTo8gWs1XVP"
   },
   "outputs": [],
   "source": [
    "#function for plotting\n",
    "def plot(x, y, sys):\n",
    "    fig, ax = plt.subplots(1,3, figsize=(15,8))\n",
    "    ax[0].set_title('Original pattern')\n",
    "    ax[0].imshow(x.reshape(L,L),cmap='Greys')\n",
    "    ax[1].set_title('Corrupted pattern')\n",
    "    ax[1].imshow(y.reshape(L,L),cmap='Greys')\n",
    "    ax[2].set_title('Recovered pattern')\n",
    "    ax[2].imshow(sys.reshape(L,L),cmap='Greys')\n",
    "    plt.show()\n",
    "\n",
    "#function to run the algorithm\n",
    "def algorithm(xi, idx, yi):  #yi is the corrupted pattern\n",
    "    sys = deterministic_hopfield(yi,30)\n",
    "    wrong_pixels = error_im(xi[idx],sys)\n",
    "    assigned_pattern = assign_pattern(xi,sys)\n",
    "    return wrong_pixels, assigned_pattern, sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLP1bdH81XVZ"
   },
   "source": [
    "## Uniform noise\n",
    "\n",
    "A uniform noise is consider as equally distributed along the pixels. To interpret the data, we define $q$ as a threshold probability. In order to corrupt the original pattern, a random number $r \\in (0,1)$ is generated with $\\textit{uniform distribution}$ for each pixel. If $r$ is less than the threshold, then that pixel is flipped. After corrupting the original pattern, the deterministic Hopfield function is called in order to produce a $\\textit{Recovered}$ one. \n",
    "\n",
    "The error with respect to the original pattern is then calculated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNdSCVyD1XVc",
    "outputId": "1d3a2e94-cf93-4f8f-823f-b41bba5e05fd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creates corrupted random data.\n",
    "def corrupt_uniform(sys, q): #q is the threshold probability\n",
    "    sys_temp = deepcopy(sys)\n",
    "    N = sys.shape[0]\n",
    "    for i in range(N):\n",
    "        r = np.random.rand()\n",
    "        if r < q:\n",
    "            sys_temp[i] *= -1\n",
    "    return sys_temp\n",
    "    \n",
    "#Testing the function\n",
    "idx = 1\n",
    "yi = corrupt_uniform(xi[idx],0.1) #we start with q = 0.1\n",
    "wrong_pixels, assigned_pattern, sys =algorithm(xi, idx, yi)\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))\n",
    "plot(xi[idx], yi, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the probability\n",
    "\n",
    "In this section we change the values of the threshold probability $q$ in order to visualize how the system behaves using both MF and NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgKouBSJ1XWX"
   },
   "outputs": [],
   "source": [
    "#set the probabilities between 0 and 1\n",
    "probabilities = np.arange(0,1,0.05)\n",
    "error = [[],[]]\n",
    "sigma = [[],[]]\n",
    "\n",
    "MF = [0,1]\n",
    "\n",
    "for i in MF:\n",
    "    if i: J = MF_coupling(xi,N)\n",
    "    else: J = R_coupling(xi,N,R)        \n",
    "    for q in probabilities:\n",
    "        supp = []\n",
    "        for n in range(100):\n",
    "            yi = corrupt_uniform(xi[idx],q)\n",
    "            wrong_pixels, assigned_pattern, sys = algorithm(xi, idx, yi)\n",
    "            supp.append(wrong_pixels)\n",
    "        supp = np.array(supp)\n",
    "        error[i].append(supp.mean())\n",
    "        sigma[i].append(supp.std())\n",
    "\n",
    "#Plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "ax1.plot(probabilities, error[0], '--bo')\n",
    "ax1.set_title('Dependence on the algorithm according to parameter $q$ with NO Mean Field')\n",
    "ax1.set_xlabel('Uniform Probability $q$')\n",
    "ax1.set_ylabel('Error')\n",
    "ax1.errorbar(probabilities, error[0], yerr = sigma[0], fmt = \"none\")\n",
    "\n",
    "ax2.plot(probabilities, error[1], '--bo')\n",
    "ax2.set_title('Dependence on the algorithm according to parameter $q$ with Mean Field')\n",
    "ax2.set_xlabel('Uniform Probability $q$')\n",
    "ax2.set_ylabel('Error')\n",
    "ax2.errorbar(probabilities, error[1], yerr = sigma[1], fmt = \"none\")        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\plot1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ti7aHXUT1XWq"
   },
   "source": [
    "From the graphs, we can deduced that with probabilty of about 20% the error remains constant in both cases, especially in the MF case. This implies that for this given probability the algorithm recover the pattern perfectly. As we expect, the error increase when more pixel are flipped. \n",
    "\n",
    "One can see that the NMF is less steep than the MF, but both have a sigmoid shape. \n",
    "\n",
    "By observing the errobars, one could notice that their behaviour is different for each interval of $q$:\n",
    "- if $q\\in (0, 0.2)$, the correct pattern is always recovered and so we have negligible errors;\n",
    "- if $q\\in (0.2, 0.8)$, since we change each pixel with $q \\sim 0.5$, we start from different patterns and so we explore a wider range of patterns (minima of the system);\n",
    "- if $q\\in (0.8,1)$ almost all the pixels are flipped and so the algorithm recovers always the same pattern, even if it is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oihy6onZ1XVl"
   },
   "source": [
    "## Gaussian noise\n",
    "\n",
    "The gaussian distribution is given by:\n",
    "$$\n",
    "\\\\\n",
    "g(x) = h \\cdot \\exp{\\left[{-\\frac{(x-\\mu)^2}{2\\sigma}}\\right]}\n",
    "\\\\\n",
    "$$\n",
    "where $h$ is the maximum height of the distribution, $\\mu$ is its mean and $\\sigma$ its standard deviation.\n",
    "\n",
    "A 2D gaussian, made by the product of two statistically independent gaussians distribution, is overlapped to the original pattern. The coordinates of the centroids were chosen so as to pick only the most relevant cases, i.e. the center of the grid and its four corners. An arbitrary value of $\\sigma$ is chosen as $L/5$ in order to have a gaussian with a probability that goes to $0$ at the edges of the grid when the centroid is in its center. \n",
    "\n",
    "Concerning the parameter $h$, we decided not to choose the normalized version of the gaussian as it would have been unsignificant to our purpose, so we made $h$ vary between 0.1 and 1 in order to have different thresholds. To obtain the corrupted patterns, a random number $r$ is generated using $\\textit{uniform distribution}$ for each pixel. If $r$ is less than the threshold, set by the value of the gaussian in the considered point, then the pixel is flipped. \n",
    "\n",
    "After corrupting the patterns, the deterministic Hopfield function is called in order to produce a $\\textit{Recovered}$ one. Furthermore, the error with respect to the original pattern is calculated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3iT4GgV1XVo"
   },
   "outputs": [],
   "source": [
    "def gaus(mu, sigma, h, x, y): #mu is a 2d array representing the position of the centroid\n",
    "                              #h is the maximum height of the gaussian\n",
    "    h = np.sqrt(h)\n",
    "    g_x = np.exp(-( (x-mu[0])**2 / ( 2.0 * sigma**2 ) ) )*h\n",
    "    g_y = np.exp(-( (y-mu[1])**2 / ( 2.0 * sigma**2 ) ) )*h\n",
    "    g = g_y * g_x\n",
    "    return g\n",
    "\n",
    "def gaus_3d(mu, sigma, h, x): #mu is a 2d array representing the position of the centroid\n",
    "                              #h is the maximum height of the gaussian\n",
    "    h = np.sqrt(h)\n",
    "    g_x = np.exp(-( (x[0]-mu[0])**2 / ( 2.0 * sigma**2 ) ) )*h\n",
    "    g_y = np.exp(-( (x[1]-mu[1])**2 / ( 2.0 * sigma**2 ) ) )*h\n",
    "    g = g_y * g_x\n",
    "    return g\n",
    "\n",
    "\n",
    "x = np.linspace(0,10)\n",
    "y = np.linspace(0,10)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = gaus_3d([5,5], 2, 0.5, [X, Y])\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='summer', alpha=0.7)\n",
    "ax.set_xlabel(\"i\", fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel(\"j\", fontweight='bold', fontsize=14)\n",
    "ax.set_zlabel(\"gaus(i,j)\", fontweight='bold', fontsize=14)            \n",
    "# plt.arrow(1,0,0,gaus([5,5], 2, 0.5, 4,4)-0.01, linewidth=1,\n",
    "#          head_width=0.2, head_length=0.01, fc='g', ec='g')\n",
    "# ax.arrow(1,1,0,-(1-gaus([5,5], 2, 0.5, 4,4)-0.01), linewidth=1,\n",
    "#          head_width=0.3, head_length=0.01, fc='r', ec='r')\n",
    "# ax.text(1+.25, 0.2, 0.2, 'No flip', fontsize=16)\n",
    "# ax.text(1+.25, 0.01, 0.2 , 'Flip', fontsize=16)\n",
    "pb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gm1O-NXC1XVx",
    "outputId": "944aa2f5-cfeb-46d4-bd06-a66593b8199c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def corrupt_norm(sys, k, h): #k represent the mean in the gaussian\n",
    "                             #h represent the height in the gaussian\n",
    "    sys_temp = deepcopy(sys)\n",
    "    sys_temp = sys_temp.reshape(L,L)\n",
    "    N = sys.shape[0]\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            g = gaus(k, L/5, h, i, j)\n",
    "            r = np.random.rand()\n",
    "            if r < g:\n",
    "                sys_temp[i][j] *= -1\n",
    "    return (sys_temp.reshape(1,N))[0]\n",
    "\n",
    "idx = 1\n",
    "centroids = [np.array([0,0]), np.array([0,L-1]), np.array([L-1,0]), np.array([L-1,L-1]), np.array([L//2, L//2])]\n",
    "heights = np.linspace(0.1,1,10)\n",
    "\n",
    "for k in centroids:\n",
    "    print('The position of the centroid is ', k,'\\n')\n",
    "    for h in heights:\n",
    "        print('The height of the distribution is ', h)\n",
    "        yi = corrupt_norm(xi[idx], k, h)\n",
    "        wrong_pixels, assigned_pattern, sys = algorithm(xi, idx, yi)\n",
    "        print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "        print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))\n",
    "        plot(xi[idx], yi, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "764VJx661XWB"
   },
   "source": [
    "<img src = \".\\Images\\gaus.jpeg\">\n",
    "\n",
    "The figure shows the original pattern, the corrupted one with the centroid of the gaussian in the middle (using $h = 1$) and the recovered one. \n",
    "It can be seen that the corrupted pattern flip some pixels in the middle as we expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error as a function of gaussian's height\n",
    "\n",
    "In this section, we compare the results using MF and NMF, studying the error of the algorithm as a function of the height $h$ of the gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79iUzlxX1XXF"
   },
   "outputs": [],
   "source": [
    "# parameters \n",
    "idx = 1\n",
    "heights = np.linspace(0,1,10)\n",
    "centroids = [np.array([0,0]), np.array([0,L-1]), np.array([L-1,0]), np.array([L-1,L-1]), np.array([L//2, L//2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-wAgchu1XXL"
   },
   "outputs": [],
   "source": [
    "# gaussian corrupt error behaviour with non mean field changing the parameter height on the gaussian\n",
    "MF = 0\n",
    "error_NMF = [[],[], [],[], []]\n",
    "sigma_NMF = [[],[], [],[], []]\n",
    "i = 0\n",
    "\n",
    "if MF: J = MF_coupling(xi,N)\n",
    "else: J = R_coupling(xi,N,R)            \n",
    "\n",
    "for k in centroids:\n",
    "    for h in heights:\n",
    "        supp = []\n",
    "        for n in range(50):\n",
    "            yi = corrupt_norm(xi[idx], k, h)\n",
    "            wrong_pixels, assigned_pattern, sys = algorithm(xi, idx, yi)\n",
    "            supp.append(wrong_pixels)\n",
    "        supp = np.array(supp)\n",
    "        error_NMF[i].append(supp.mean())\n",
    "        sigma_NMF[i].append(supp.std())\n",
    "    i +=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3VOWmJI1XXR"
   },
   "outputs": [],
   "source": [
    "# gaussian corrupt error behaviour with mean field changing the parameter height on the gaussian\n",
    "MF = 1\n",
    "error_MF = [[],[], [],[], []]\n",
    "sigma_MF = [[],[], [],[], []]\n",
    "i = 0\n",
    "\n",
    "if MF: J = MF_coupling(xi,N)\n",
    "else: J = R_coupling(xi,N,R)\n",
    "        \n",
    "for k in centroids:\n",
    "    for h in heights:\n",
    "        supp = []\n",
    "        for n in range(50):\n",
    "            yi = corrupt_norm(xi[idx], k, h)\n",
    "            wrong_pixels, assigned_pattern, sys = algorithm(xi, idx, yi)\n",
    "            supp.append(wrong_pixels)\n",
    "        supp = np.array(supp)\n",
    "        error_MF[i].append(supp.mean())\n",
    "        sigma_MF[i].append(supp.std())\n",
    "    i +=1  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mLqKvnE1XXV"
   },
   "outputs": [],
   "source": [
    "#plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 15))\n",
    "\n",
    "ax1.plot(heights, error_NMF[0], '--bo', label ='Error behaviour in centroid [0,0]')\n",
    "ax1.plot(heights, error_NMF[1], '-ro', label ='Error behaviour in centroid [0,L-1]')\n",
    "ax1.plot(heights, error_NMF[2], '--go', label ='Error behaviour in centroid [L-1,0]')\n",
    "ax1.plot(heights, error_NMF[3], '--yo', label ='Error behaviour in centroid [L-1,L-1]')\n",
    "ax1.plot(heights, error_NMF[4], '--co', label ='Error behaviour in middle centroid')\n",
    "ax1.set_title('Dependence on the algorithm according to height $h$ of the gaussian with NO Mean Field')\n",
    "ax1.set_xlabel('Heights $h$')\n",
    "ax1.set_ylabel('Error');\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(heights, error_MF[0], '--b', label ='Error behaviour in centroid [0,0]')\n",
    "ax2.plot(heights, error_MF[1], '--ro', label ='Error behaviour in centroid [0,L-1]')\n",
    "ax2.plot(heights, error_MF[2], '--go', label ='Error behaviour in centroid [L-1,0]')\n",
    "ax2.plot(heights, error_MF[3], '--yo', label ='Error behaviour in centroid [L-1,L-1]')\n",
    "ax2.plot(heights, error_MF[4], '--co', label ='Error behaviour in middle centroid')\n",
    "ax2.set_title('Dependence on the algorithm according to height $h$ of the gaussian with Mean Field')\n",
    "ax2.set_xlabel('Heights $h$')\n",
    "ax2.set_ylabel('Error');\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/gaussian_corruption_h.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MF model is not influenced by the gaussian noise, as it is able to get a broad picture and not be mislead by local changes.\n",
    "\n",
    "On the contrary, the NMF is affected, especially if the centroid coincide with the middle of the grid. The error increase with the gaussian height, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NOJZSbd1XWE"
   },
   "source": [
    "## Cauchy noise\n",
    "\n",
    "The Cauchy distribution is given by:\n",
    "$$\n",
    "\\\\\n",
    "\\mathcal{C}(x) = h \\cdot \\left(1 + \\left(\\frac{x- x_0}{\\gamma}\\right)^2 \\right)^{-1}\n",
    "\\\\\n",
    "$$\n",
    "where $h$ is the maximum height of the distribution, $x_{0}$ specifies the location of the peak of the distribution, and $\\gamma$ is the scale parameter which specifies the half-width at half-maximum (HWHM).\n",
    "\n",
    "We chose $\\gamma$ arbitrarily ($\\gamma = L/5$) and made the peak $x_0$ of the distribution and its height $h$ variable, like the parameters $\\sigma$ and $\\mu$ of the gaussian distribution.\n",
    "\n",
    "We followed the same approach we used for the gaussian noise: we overlapped a 2-dimensional Cauchy distribution, and we used it to have a variable threshold for each pixel. Then we generated corrupted patterns, recovered them and evaluated the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Alue-hBu1XWF"
   },
   "outputs": [],
   "source": [
    "def cauchy(mu, gamma, h, x, y):#mu is a 2d array representing the position of the median\n",
    "                               #h is the maximum height of the distribution     \n",
    "    h = np.sqrt(h)\n",
    "    c_x = h/(1 + (((x-mu[0]))/gamma)**2)\n",
    "    c_y = h/(1 + (((y-mu[1]))/gamma)**2)\n",
    "    c = c_y * c_x\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CxNq0-E1XWO",
    "outputId": "a765b0b3-1d24-4e2b-faff-4d98e7928ba9"
   },
   "outputs": [],
   "source": [
    "def corrupt_cauchy(sys, k, h): #k represent the mean in the cauchy distribution\n",
    "                               #h represent the height in the cauchy distribution\n",
    "    sys_temp = deepcopy(sys)\n",
    "    sys_temp = sys_temp.reshape(L,L)\n",
    "    N = sys.shape[0]\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            g = cauchy(k, L/5, h, i, j)\n",
    "            r = np.random.rand()\n",
    "            if r < g:\n",
    "                sys_temp[i][j] *= -1\n",
    "    return (sys_temp.reshape(1,N))[0]\n",
    "\n",
    "idx = 1\n",
    "centroids = [np.array([0,0]), np.array([0,L-1]), np.array([L-1,0]), np.array([L-1,L-1]), np.array([L//2, L//2])]\n",
    "heights = np.linspace(0.1,1,10)\n",
    "\n",
    "for k in centroids:\n",
    "    print('The position of the median is ', k,'\\n')\n",
    "    for h in heights:\n",
    "        print('The height of the distribution is ', h)\n",
    "        yi = corrupt_cauchy(xi[idx], k, h)\n",
    "        wrong_pixels, assigned_pattern, sys = algorithm(xi, idx, yi)\n",
    "        print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "        print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))\n",
    "        plot(xi[idx], yi, sys)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCiqkdEH1XWW"
   },
   "source": [
    "<img src = \".\\Images\\corrupted_cauchy.png\">\n",
    "\n",
    "The figure shows the original pattern, the corrupted one with the centroid of the cauchy in the middle of the grid (using $h = 1$) and the recovered one. \n",
    "\n",
    "As we expected, the flipped pixel are mostly in the middle, following the tendency of the Cauchy distribution, which is more spread than the gaussian one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error as a function of Cauchy curve's height\n",
    "\n",
    "In this section, we compare the results using MF and NMF, studying the error of the algorithm as a function of the height $h$ of the Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiWF75NO1XXa"
   },
   "outputs": [],
   "source": [
    "# Behaviour of Cauchy distribution changing height using MF\n",
    "MF = 1\n",
    "error_MF_cauchy = [[],[], [],[], []]\n",
    "i = 0\n",
    "\n",
    "if MF: J = MF_coupling(xi,N)\n",
    "else: J = R_coupling(xi,N,R)\n",
    "            \n",
    "for k in centroids:\n",
    "    for h in heights:\n",
    "        supp = []\n",
    "        for n in range(50):\n",
    "            yi = corrupt_cauchy(xi[idx], k, h)\n",
    "            wrong_pixels, assigned_pattern, sys = algorithm(xi, idx, yi)\n",
    "            supp.append(wrong_pixels)\n",
    "        supp = np.array(supp)\n",
    "        error_MF_cauchy[i].append(supp.mean())\n",
    "    i +=1 \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TW8Re5bX1XXf"
   },
   "outputs": [],
   "source": [
    "# Behaviour of Cauchy distribution changing height using NMF\n",
    "MF = 0\n",
    "error_NMF_cauchy = [[],[], [],[], []]\n",
    "i = 0\n",
    "\n",
    "if MF: J = MF_coupling(xi,N)\n",
    "else: J = R_coupling(xi,N,R)\n",
    "            \n",
    "for k in centroids:\n",
    "    for h in heights:\n",
    "        supp = []\n",
    "        for n in range(50):\n",
    "            yi = corrupt_cauchy(xi[idx], k, h)\n",
    "            wrong_pixels, assigned_pattern, sys = algorithm(xi, idx, yi)\n",
    "            supp.append(wrong_pixels)\n",
    "        supp = np.array(supp)\n",
    "        error_NMF_cauchy[i].append(supp.mean())\n",
    "    i +=1 \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YTjANV81XXj"
   },
   "outputs": [],
   "source": [
    "#plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 15))\n",
    "\n",
    "ax1.plot(heights, error_NMF_cauchy[0], '--bo', label ='Error behaviour in centroid [0,0]')\n",
    "ax1.plot(heights, error_NMF_cauchy[1], '--ro', label ='Error behaviour in centroid [0,L-1]')\n",
    "ax1.plot(heights, error_NMF_cauchy[2], '--go', label ='Error behaviour in centroid [L-1,0]')\n",
    "ax1.plot(heights, error_NMF_cauchy[3], '--yo', label ='Error behaviour in centroid [L-1,L-1]')\n",
    "ax1.plot(heights, error_NMF_cauchy[4], '--co', label ='Error behaviour in middle centroid')\n",
    "ax1.set_title('Dependence on the algorithm according to height $h$ of the cauchy with NO Mean Field')\n",
    "ax1.set_xlabel('Heights $h$')\n",
    "ax1.set_ylabel('Error');\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(heights, error_MF_cauchy[0], '--bo', label ='Error behaviour in centroid [0,0]')\n",
    "ax2.plot(heights, error_MF_cauchy[1], '--ro', label ='Error behaviour in centroid [0,L-1]')\n",
    "ax2.plot(heights, error_MF_cauchy[2], '--go', label ='Error behaviour in centroid [L-1,0]')\n",
    "ax2.plot(heights, error_MF_cauchy[3], '--yo', label ='Error behaviour in centroid [L-1,L-1]')\n",
    "ax2.plot(heights, error_MF_cauchy[4], '--co', label ='Error behaviour in middle centroid')\n",
    "ax2.set_title('Dependence on the algorithm according to height $h$ of the cauchy with Mean Field')\n",
    "ax2.set_xlabel('Heights $h$')\n",
    "ax2.set_ylabel('Error');\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/cauchy_corruption_h.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqYH12fj1XXn"
   },
   "source": [
    "Again, just like the gaussian case, the MF model is not influenced by the Cauchy noise, as it is able to get a broad picture and not be mislead by local changes.\n",
    "\n",
    "On the contrary, the NMF is even more affected, especially if the centroid coincide with the middle of the grid. The Caushy distribution is indeed wider and hence impacts on more pixels than the gaussian. Anyway, the error increase with the Cauchy curve height, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montecarlo simulation\n",
    "\n",
    "Montecarlo simulations are computational tools that use pseudorandom numbers to feign the evolution of a physical system. As we have stated before, a really interesting feature of the Hopfield model is that it gives us the possibility of mapping a neuron system into a spin glasses system, hence giving us the possibility to employ such techniques. \n",
    "\n",
    "For this particular task we aim at finding the ground state of a spin system using the Metropolis algorithm as update rule. \n",
    "\n",
    "Given the system energy $\\mathcal{H}$, we randomly flip a spin of the system and we compute the difference in energy $\\Delta\\mathcal{H}=\\mathcal{H}_{f}-\\mathcal{H}_{i}$. Then accept the updated system with the following probability:\n",
    "$$\n",
    "\\begin{equation}\n",
    "p_{acc}= \\min\\left( 1, e^{-\\beta\\Delta\\mathcal{H}} \\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\beta=(K_{b}T)^{-1}$ is the inverse of the temperature of the system. In this way if the energy is decreasing, we update the system with probability 1, while if it is greater, with a probability inversely proportional to the increase in energy.\n",
    "From the theory we know that the critical temperature, i.e. the highest temperature at which the system is trapped in a minimum, is $T=1/K_b$, hence $\\beta=1$.\n",
    "\n",
    "In this section we will cover:\n",
    "\n",
    "- the stability of the Montecarlo methods;\n",
    "- the improvement in the time implementation of Montecarlo;\n",
    "- the dependance of the algorithm's performances w.r.t. the type of noise;\n",
    "- the dependance of the algorithm's performances w.r.t. the number of neighbors for the NON-MF version;\n",
    "- the distinguish criteria between similar patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(J,sys):  #energy of the system\n",
    "    N = sys.shape[0]\n",
    "    energy = 0\n",
    "    for i in range(N):\n",
    "        for j in range(i):\n",
    "            energy -= J[i][j]*sys[i]*sys[j]\n",
    "    return energy\n",
    "\n",
    "# difference if energy when flipping the spin k\n",
    "def dH(J, sys, k): \n",
    "    N = sys.shape[0]\n",
    "    dh = 0\n",
    "    for i in range(N):\n",
    "        dh += sys[i]*J[k][i]\n",
    "    dh *= 2*sys[k]\n",
    "    return dh\n",
    "\n",
    "#metropolis update rule\n",
    "def metropolis(J, sys1, k, B): # B = 1/kbT temperature of the system\n",
    "    sys = deepcopy(sys1)\n",
    "    dh = dH(J, sys, k)\n",
    "    r = np.random.rand() # random variable between (0,1)\n",
    "    if r < np.exp( -B*dh ):\n",
    "        sys[k] = -sys[k]\n",
    "    return sys\n",
    "\n",
    "#actual algorithm\n",
    "def montecarlo(J, sys1, B, t ): # t number of iteration of the montecarlo\n",
    "    sys = deepcopy(sys1)\n",
    "    N = sys.shape[0]\n",
    "    for i in range(t):\n",
    "        k = np.random.randint(0,N)\n",
    "        sys = metropolis(J, sys, k, B)\n",
    "    return sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability for Mean Field Montecarlo method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we study the stability of the Montecarlo method w.r.t. its characteristical parameters: $\\beta$, $t$ number of iterations . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = random_patterns(N,p)\n",
    "J = MF_coupling(xi,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(10)\n",
    "\n",
    "sys = montecarlo(J,xi[idx], 10, 1000)\n",
    "wrong_pixels = error_im(xi[idx],sys)\n",
    "assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,6))\n",
    "ax[0].set_title('Pattern')\n",
    "ax[0].imshow(xi[idx].reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Finishing configuration')\n",
    "ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature analysis\n",
    "\n",
    "We fix the number of iterations to $t=1000$ and size of the system to $N=100$ ($L=10$), while changing the temperature in a range $\\beta=[0,1,10,25,50,100]$. For each point we consider the average over $100$ loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [0, 1, 10, 25, 50, 100]\n",
    "err_beta_mean = []\n",
    "err_beta_std = []\n",
    "success = []\n",
    "for b in beta:\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        sys = montecarlo(J,xi[idx], b, 1000)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_beta_mean.append( errors.mean() )\n",
    "    err_beta_std.append( errors.std() )\n",
    "    success.append(correct)\n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of T')\n",
    "ax[0].set_xlabel('Beta [$J^{-1}$]')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(beta, err_beta_mean, '--bo', label='Error')\n",
    "ax[0].errorbar(beta,err_beta_mean, yerr=err_beta_std,ecolor='red', label='Fluctuation of error',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of T')\n",
    "ax[1].set_xlabel('Beta [$J^{-1}$]')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(beta, success, '--ro', label='Data')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \".\\Images\\m_avge_beta.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how a higher value of $\\beta$ leads to  higher probability of success, and a $\\beta<10$ leads to totally unacceptable results.\n",
    "\n",
    "Even if $\\beta>25$ seems to produce better results, we have to think about the functioning of the algorithm: with a very high beta, the probability of flipping a \"wrong\" spin is $\\sim0$ and, starting from a minimum, it probably means that it does not flip a spin at all. So, for the following analisys we will use $\\beta=10$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of iteration analysis\n",
    "\n",
    "Here we analyze the stability of the algorithm w.r.t. the number of iterations, chosen in a range $t=[10,25,50,100,200,500,750,1000,1250,1500,1750,2000]$, while the other parameters are kept fixed ($\\beta=[10,25]$ and $L=10$). For each point we consider the average over $100$ loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=[10,25,50,100,200,500,750,1000,1250,1500,1750,2000]\n",
    "\n",
    "# Beta = 10\n",
    "err_t_mean = []\n",
    "err_t_std = []\n",
    "success_t = []\n",
    "for t in iterations:\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        sys = montecarlo(J,xi[idx], 10, t)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_t_mean.append( errors.mean() )\n",
    "    err_t_std.append( errors.std() )\n",
    "    success_t.append(correct)\n",
    "    \n",
    "# Beta = 25\n",
    "err_t1_mean = []\n",
    "err_t1_std = []\n",
    "success_t1 = []\n",
    "for t in iterations:\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        sys = montecarlo(J,xi[idx], 25, t)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_t1_mean.append( errors.mean() )\n",
    "    err_t1_std.append( errors.std() )\n",
    "    success_t1.append(correct)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of iterations')\n",
    "ax[0].set_xlabel('Number of iteration t')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(iterations, err_t_mean, '--bo', label='Error with beta=10')\n",
    "ax[0].errorbar(iterations,err_t_mean, yerr=err_t_std,ecolor='blue',fmt='none')\n",
    "ax[0].plot(iterations, err_t1_mean, '--go', label='Error with beta=25')\n",
    "ax[0].errorbar(iterations,err_t1_mean, yerr=err_t1_std,ecolor='green',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of the number of iterations t')\n",
    "ax[1].set_xlabel('Number of iteration t')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(iterations, success_t, '--bo', label='Data with beta=10')\n",
    "ax[1].plot(iterations, success_t1, '--go', label='Data with beta=25')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \".\\Images\\m_s_iterations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can expect, the number of iterations, in this case, actually raises the error of the algorithm. It is predictable since we start from the correct pattern, hence adding iterations only increase the probability of flipping some random spin. \n",
    "\n",
    "We can observe from the graphs that a $\\beta=10$ leads to a unstable situation: in fact, the error reaches a plateau, but not the probability of success.\n",
    "\n",
    "On the other hand, with $\\beta=25$, both error and probability of success seems to reach a plateau. So in the following we will use $25$ as the value of $\\beta$.\n",
    "\n",
    "Given these results, we decide to set the optimal $t$ as the beginning of the plateau, namely $t=1000$, since it is the minimun number of iteration for which convergence is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corruption with uniform noise\n",
    "\n",
    "As the stability of the algorithm has been thoroughly studied, we proceed to corrupt and recover patterns, starting from a uniform distribution along the grid of height $q$.\n",
    "\n",
    "We only used this kind of noise because it was the most significant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "\n",
    "yi = corrupt_uniform(xi[idx], 0.2)\n",
    "\n",
    "sys = montecarlo(J,yi, 10, 100)\n",
    "wrong_pixels = error_im(xi[idx],sys)\n",
    "assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,6))\n",
    "ax[0].set_title('Pattern')\n",
    "ax[0].imshow(xi[idx].reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Finishing configuration')\n",
    "ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature analysis\n",
    "\n",
    "First of all, it is needed to be stated that in this section we start from a corrupted pattern. This observation seems trivial, but indeed it is not, because the starting configuration is not a minimum and as a consequence some iterations are required to reach one of the original patterns.\n",
    "\n",
    "Hence, we study the dependance of the algorithm on the temperature, with uniformly corrupted patterns using $q=0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [0, 1, 10, 25, 50, 100]\n",
    "err_beta_mean_c = []\n",
    "err_beta_std_c = []\n",
    "success_beta_c = []\n",
    "\n",
    "for b in beta:\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        yi = corrupt_uniform(xi[idx], 0.2)\n",
    "        sys = montecarlo(J, yi, b, 1000)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_beta_mean_c.append( errors.mean() )\n",
    "    err_beta_std_c.append( errors.std() )\n",
    "    success_beta_c.append(correct)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of T with uniform noise')\n",
    "ax[0].set_xlabel('Beta [$J^{-1}$]')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(beta, err_beta_mean_c, '--bo', label='Error')\n",
    "ax[0].errorbar(beta,err_beta_mean_c, yerr=err_beta_std_c,ecolor='red', label='Fluctuation of error',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of T with uniform noise')\n",
    "ax[1].set_xlabel('Beta [$J^{-1}$]')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(beta, success_beta_c, '--ro', label='Data')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \".\\Images\\m_c_beta.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing these graphs, a further confirmation of the best choice for the temeperature parameter $\\beta$ can be found, just as stated in the stability section. \n",
    "\n",
    "Consequently, from now on we will use $\\beta = 25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of iteration analysis\n",
    "\n",
    "Just as the previously, we analyze the performances of the algorithm w.r.t. the number of iterations, chosen in a range $t=[10,25,50,100,200,500,750,1000,1250,1500,1750,2000]$, while the other parameters are kept fixed ($\\beta=25$, $L=10$, $q=0.2$). For each point we consider the average over $100$ loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [10,25,50,100,200,500,750,1000,1250,1500,1750,2000]\n",
    "err_t_mean_c = []\n",
    "err_t_std_c = []\n",
    "success_t_c = []\n",
    "\n",
    "for t in iterations:\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        yi = corrupt_uniform(xi[idx], 0.2)\n",
    "        sys = montecarlo(J, yi, 25, t)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_t_mean_c.append( errors.mean() )\n",
    "    err_t_std_c.append( errors.std() )\n",
    "    success_t_c.append(correct)\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of iterations')\n",
    "ax[0].set_xlabel('Number of iteration t')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(iterations, err_t_mean_c, '--bo', label='Errors')\n",
    "ax[0].errorbar(iterations,err_t_mean_c, yerr=err_t_std_c, ecolor='blue',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of the number of iterations t')\n",
    "ax[1].set_xlabel('Number of iteration t')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(iterations, success_t_c, '--bo', label='Data')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\m_c_iter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to outline a clear trend in both graphs. Probably, increasing the number of tests and observations could lead to better results. \n",
    "In any case, the graphs seem to reach an oscillatory behaviour after $t=1000$, this is particularly evident by observing the tendency of the probability of success. \n",
    "\n",
    "For this reason we choose $t=500$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying the probability $q$ \n",
    "\n",
    "Last but not least, we analyse the behaviour of the algorithm with $t=500$, $\\beta=25$, changing the value of  $q \\in [0.1,1]$ in steps of $0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.linspace(0,1,10)\n",
    "err_q_mean_c = []\n",
    "err_q_std_c = []\n",
    "success_q_c = []\n",
    "\n",
    "for q in prob:\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        yi = corrupt_uniform(xi[idx], q)\n",
    "        sys = montecarlo(J, yi, 25, 500)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_q_mean_c.append( errors.mean() )\n",
    "    err_q_std_c.append( errors.std() )\n",
    "    success_q_c.append(correct)\n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of the flipping probability')\n",
    "ax[0].set_xlabel('Flipping probability q')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(prob, err_q_mean_c, '--bo', label='Errors')\n",
    "ax[0].errorbar(prob,err_q_mean_c, yerr=err_q_std_c, ecolor='blue',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of the flipping probabiity')\n",
    "ax[1].set_xlabel('Flipping probability q')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(prob, success_q_c, '--bo', label='Data')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\m_c_unif.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that, even if the error is still acceptable, the maximum value of the noise that we can accept to have good performances is $q=0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Mean Field Montecarlo\n",
    "\n",
    "Up to now we used only the MF version, where all the spins interacts with each other. We will now use a version of the algorithm where the spins interacts only within a small range, indicated with $R$. <br>\n",
    "We will not repeat the analysis of the dependance on temperature, and so we will use $\\beta=25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = random_patterns(N,p)\n",
    "J = R_coupling(xi,N,3)\n",
    "\n",
    "idx = 3\n",
    "\n",
    "sys = montecarlo(J,xi[idx], 25, 1000)\n",
    "wrong_pixels = error_im(xi[idx],sys)\n",
    "assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,6))\n",
    "ax[0].set_title('Pattern')\n",
    "ax[0].imshow(xi[idx].reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Finishing configuration')\n",
    "ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability for the Non Mean Field Montecarlo\n",
    "\n",
    "In this section we study the stability of the algorithm as a function of the number of neighbors, $R = [1,2,3,4,5]$ and two different number of iterations, $t=[500,1000]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [1, 2, 3, 4, 5]\n",
    "err_r_mean_c = []\n",
    "err_r_std_c = []\n",
    "success_r_c = []\n",
    "\n",
    "for r in R:\n",
    "    J = R_coupling(xi,N,r)\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        sys = montecarlo(J, xi[idx], 25, 500)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_r_mean_c.append( errors.mean() )\n",
    "    err_r_std_c.append( errors.std() )\n",
    "    success_r_c.append(correct)\n",
    "    \n",
    "err_r1_mean_c = []\n",
    "err_r1_std_c = []\n",
    "success_r1_c = []\n",
    "\n",
    "for r in R:\n",
    "    J = R_coupling(xi,N,r)\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        sys = montecarlo(J, xi[idx], 25, 1000)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_r1_mean_c.append( errors.mean() )\n",
    "    err_r1_std_c.append( errors.std() )\n",
    "    success_r1_c.append(correct)\n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of nearest neighbors')\n",
    "ax[0].set_xlabel('Number of nearest neighbors R')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(R, err_r_mean_c, '--bo', label='Error with t=500')\n",
    "ax[0].errorbar(R,err_r_mean_c, yerr=err_r_std_c,ecolor='blue',fmt='none')\n",
    "ax[0].plot(R, err_r1_mean_c, '--go', label='Error with t=1000')\n",
    "ax[0].errorbar(R,err_r1_mean_c, yerr=err_r1_std_c,ecolor='green',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of nearest neighbors')\n",
    "ax[1].set_xlabel('Number of nearest neighbors R')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(R, success_r_c, '--bo', label='Data with t=500')\n",
    "ax[1].plot(R, success_r1_c, '--go', label='Data with t=1000')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\m_nMF_r.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, increasing the number of nearest neighbors enhances the performances of the algorithm, and makes the NMF model approach to the MF one.\n",
    "\n",
    "Actually, the error and the probability of success with $R=5$ are even better than the ones obtained thanks to the MF model. Another important thing to notice is that the algorithm for different $t$ converges to the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corruption with noise\n",
    "\n",
    "We will see now how the noise affect the performances of the NMF Montecarlo method. This analysis is very interesting because we will see different behaviour between gaussian and uniform noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform noise\n",
    "\n",
    "Here we analyse how different threshold probabilities $q$ affect the performances, with $\\beta=25$, $R=[3,5]$ and $t=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "yi = corrupt_uniform(xi[idx], 0.35)\n",
    "\n",
    "sys = montecarlo(J,yi, 100, 100)\n",
    "\n",
    "wrong_pixels = error_im(xi[idx],sys)\n",
    "assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,6))\n",
    "ax[0].set_title('Pattern')\n",
    "ax[0].imshow(xi[idx].reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Finishing configuration')\n",
    "ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = random_patterns(N,p)\n",
    "prob = np.linspace(0,1,10)\n",
    "err_rq_mean_c = []\n",
    "err_rq_std_c = []\n",
    "success_rq_c = []\n",
    "\n",
    "# R = 3\n",
    "for q in prob:\n",
    "    J = R_coupling(xi,N,3)\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        yi = corrupt_uniform(xi[idx], q)\n",
    "        sys = montecarlo(J, yi, 25, 1000)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_rq_mean_c.append( errors.mean() )\n",
    "    err_rq_std_c.append( errors.std() )\n",
    "    success_rq_c.append(correct)\n",
    "    \n",
    "err_rq1_mean_c = []\n",
    "err_rq1_std_c = []\n",
    "success_rq1_c = []\n",
    "\n",
    "# R = 5\n",
    "for q in prob:\n",
    "    J = R_coupling(xi,N,5)\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        yi = corrupt_uniform(xi[idx], q)\n",
    "        sys = montecarlo(J, yi, 25, 1000)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_rq1_mean_c.append( errors.mean() )\n",
    "    err_rq1_std_c.append( errors.std() )\n",
    "    success_rq1_c.append(correct)\n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of flipping probability')\n",
    "ax[0].set_xlabel('Flipping probability q')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(prob, err_rq_mean_c, '--bo', label='Error with R=3')\n",
    "ax[0].errorbar(prob,err_rq_mean_c, yerr=err_rq_std_c,ecolor='blue',fmt='none')\n",
    "ax[0].plot(prob, err_rq1_mean_c, '--go', label='Error with R=5')\n",
    "ax[0].errorbar(prob,err_rq1_mean_c, yerr=err_rq1_std_c,ecolor='green',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of flipping probability')\n",
    "ax[1].set_xlabel('Flipping probability q')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(prob, success_rq_c, '--bo', label='Data with R=3')\n",
    "ax[1].plot(prob, success_rq1_c, '--go', label='Data with R=5')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\m_nMF_unif.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behaviour outlined in the graphs is similar to the one of the MF, especially with the bigger number of neighbors ($R=5$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian noise\n",
    "\n",
    "We corrupted the original patterns with a gaussian distribution; in this section we use $h\\in[0,1]$ with steps of $0.1$ as gaussian heights, and the different centroid positions (corner or center of the grid).\n",
    "\n",
    "We study the behaviour of the corrupted system with $\\beta=25$, $R=5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = random_patterns(N,p)\n",
    "heights = np.linspace(0,1,10)\n",
    "err_rh_mean_c = []\n",
    "err_rh_std_c = []\n",
    "success_rh_c = []\n",
    "\n",
    "for h in heights:\n",
    "    J = R_coupling(xi,N,5)\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        yi = corrupt_norm(xi[idx], np.array([0,0]), h)\n",
    "        sys = montecarlo(J, yi, 25, 1000)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_rh_mean_c.append( errors.mean() )\n",
    "    err_rh_std_c.append( errors.std() )\n",
    "    success_rh_c.append(correct)\n",
    "    \n",
    "err_rh1_mean_c = []\n",
    "err_rh1_std_c = []\n",
    "success_rh1_c = []\n",
    "for h in heights:\n",
    "    J = R_coupling(xi,N,5)\n",
    "    errors = []\n",
    "    correct = 0\n",
    "    for j in range(100):\n",
    "        idx = np.random.randint(10)\n",
    "        yi = corrupt_norm(xi[idx], np.array([L//2, L//2]), h)\n",
    "        sys = montecarlo(J, yi, 25, 1000)\n",
    "        wrong_pixels = error_im(xi[idx],sys)\n",
    "        assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "        if idx == assigned_pattern: correct += 1\n",
    "        errors.append(wrong_pixels) \n",
    "        \n",
    "    correct /= 100\n",
    "    errors = np.array(errors)\n",
    "    err_rh1_mean_c.append( errors.mean() )\n",
    "    err_rh1_std_c.append( errors.std() )\n",
    "    success_rh1_c.append(correct)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].set_title('Errors as function of the height of the gaussian')\n",
    "ax[0].set_xlabel('Gaussian height h')\n",
    "ax[0].set_ylabel('Average error')\n",
    "ax[0].plot(heights, err_rh_mean_c, '--bo', label='Error with centroid in [0,0]')\n",
    "ax[0].errorbar(heights,err_rh_mean_c, yerr=err_rh_std_c,ecolor='blue',fmt='none')\n",
    "ax[0].plot(heights, err_rh1_mean_c, '--go', label='Error with centroid in [L/2,L/2]')\n",
    "ax[0].errorbar(heights,err_rh1_mean_c, yerr=err_rh1_std_c,ecolor='green',fmt='none')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Probability of success as function of the height of the gaussian')\n",
    "ax[1].set_xlabel('Gaussian height h')\n",
    "ax[1].set_ylabel('Probability of success')\n",
    "ax[1].plot(heights, success_rh_c, '--bo', label='Data with centroid in [0,0]')\n",
    "ax[1].plot(heights, success_rh1_c, '--go', label='Data with centroid in [L/2,L/2]')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\m_nMF_heigh_gauss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of success decrease with the increase of the height of the gaussian if the centroid is in the middle of the grid: despite this, there is still a 60% of probability of recognising the correct pattern even in the worst case scenario ($h=1$).\n",
    "\n",
    "On the other hand, the gaussian height does not affect too much the performance of the algorithm if the distribution is centered in one of the corners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Time analisys\n",
    "\n",
    "The aim of this section is to quantify the time implementation of the Montecarlo methods, both MF and not MF, in function of the size of the system N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't run this cell but it is the very last thing to do\n",
    "lenght = np.array([5, 10, 15, 20, 25, 28, 30])\n",
    "size = lenght**2\n",
    "time_n_mean = []\n",
    "time_n_std = []\n",
    "for n in size:\n",
    "    xi = random_patterns(n,p)\n",
    "    J =  MF_coupling(xi,n)\n",
    "    time1 = []\n",
    "    for j in range(50):\n",
    "        start = time.time()\n",
    "        idx = np.random.randint(10)\n",
    "        sys = montecarlo(J, xi[idx], 25, 1000)\n",
    "        end = time.time()\n",
    "        time1.append(end-start)\n",
    "        \n",
    "    time1 = np.array(time1)\n",
    "    time_n_mean.append( time1.mean() )\n",
    "    time_n_std.append( time1.std() )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.set_title('Time implementation as function of the size of the system')\n",
    "ax.set_xlabel('Size of the system L')\n",
    "ax.set_ylabel('Average time implementation')\n",
    "ax.plot(lenght, time_n_mean, '--bo', label='Montecarlo')\n",
    "ax.errorbar(lenght,time_n_mean, yerr=time_n_std,ecolor='blue',fmt='none')\n",
    "ax.plot(np.arange(10,28,4), times[0], '--ro', label='Deterministic Hopfield')\n",
    "ax.legend()\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\m_time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the deterministic model and the Montecarlo one, we can see that the time implementation is way more efficient than the deterministic one, as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguish criteria of similar patterns\n",
    "\n",
    "Finally, after discussing the performances and the advantages of the Hopfield model, we try to understand its limits.\n",
    "Therefore, in this section we will study when and why the algorithm cannot distinguish some kind of patterns.\n",
    "\n",
    "We define a set of $\\textit{p similar patterns}$ as a group of $p$ patterns which cannot be distinguished by the Hopfield model.\n",
    "\n",
    "Our first guess is that the similarity between two patterns is inversely proportional to the distance among the nearest different pixels.\n",
    "Hence, we define a new function, $w_{sim}$, based on this hypothesis, and try to implement the model on the two most similar patterns so defined: a chessboard configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we work with this parameters \n",
    "L = 10\n",
    "N = L**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_sim(sys11, sys22, alpha): \n",
    "    sys1 = deepcopy(sys11)\n",
    "    sys2 = deepcopy(sys22)\n",
    "    N = sys1.shape[0]\n",
    "    L = int(np.sqrt(N))\n",
    "    sys1 = sys1.reshape(L,L)\n",
    "    sys2 = sys2.reshape(L,L)\n",
    "    wrong1 = [] #wrong black pixels in sys1\n",
    "    wrong2 = [] # in sys2\n",
    "    min_dist = [] \n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            if sys1[i][j] == sys2[i][j]: continue\n",
    "            elif sys1[i][j]== 1 : # if black in sys1\n",
    "                wrong1.append( np.array([i,j]) )\n",
    "            elif sys2[i][j]== 1: # if black in sys2\n",
    "                wrong2.append( np.array([i,j]) )\n",
    "    # compute distances between wrong pixels of the two systems\n",
    "    for i in wrong1:\n",
    "        distances = []\n",
    "        for j in wrong2:\n",
    "            if np.linalg.norm(i-j) !=0 :\n",
    "                distances.append( np.linalg.norm(i-j) )\n",
    "        min_dist.append( np.array(distances).min() )\n",
    "    min_dist = np.array(min_dist)\n",
    "    err =  (1/(min_dist)**alpha).sum()\n",
    "    err = err/((N+1)//2) #normalization due to worst case scenario (a chessboard with more black cells)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chessboard\n",
    "chess = [np.zeros(N)-1 for j in range(2)]\n",
    "c = np.array([np.zeros((L,L))-1 for j in range(2)])\n",
    "\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        c[i][::2, ::2] = 1\n",
    "        c[i][1::2, 1::2] = 1\n",
    "    else:\n",
    "        c[i][1::2, ::2] = 1\n",
    "        c[i][::2, 1::2] = 1\n",
    "    chess[i] = c[i].reshape(1,N)[0]\n",
    "\n",
    "err_chess = []   \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "ax[0].set_title('Pattern 1')\n",
    "ax[0].imshow(chess[0].reshape(L,L),cmap='Greys')\n",
    "ax[1].set_title('Pattern 2')\n",
    "ax[1].imshow(chess[1].reshape(L,L),cmap='Greys')\n",
    "plt.show()    \n",
    "    \n",
    "for i in chess:    \n",
    "    err_chess.append(w_sim(chess[0],i,2))\n",
    "\n",
    "print(\"Similarity between the two patterns: \", err_chess[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we try our algorithm to see if the two chessboards are distinguishable.\n",
    "\n",
    "chess = np.array(chess)\n",
    "ind = [0,1]\n",
    "\n",
    "for idx in ind: \n",
    "    for MF in range(2):\n",
    "        if MF: J = MF_coupling(chess,N)\n",
    "        else: J = R_coupling(chess,N,R)\n",
    "\n",
    "        sys = deepcopy(chess[idx])\n",
    "\n",
    "        sys = montecarlo(J, sys, 25, 500)\n",
    "        wrong_pixels = error_im(chess[idx],sys)\n",
    "        assigned_pattern = assign_pattern(chess,sys)\n",
    "\n",
    "#         fig, ax = plt.subplots(1,2, figsize = (8,5))\n",
    "#         ax[0].set_title('Pattern')\n",
    "#         ax[0].imshow(chess[idx].reshape(L,L), cmap='Greys')\n",
    "#         ax[1].set_title('Finishing configuration')\n",
    "#         ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "#         plt.show()\n",
    "        print('The error of the algorithm using MF = %i is %f' %(MF, wrong_pixels))\n",
    "        print('The algorithm recognised the pattern %i and the correct pattern is %i\\n' %(assigned_pattern,idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the contrary of what we thought, the distance criteria is not a good one to define similar patterns, as our algorithm can distinguish with no error the two chessboards. \n",
    "\n",
    "Hence, a new approach needs to be applied: an intuitive idea is that two patterns might be similar when they have only a few different pixels. As a consequence, we try with some regularly shaped patterns where the number of different pixels can be easily counted.\n",
    "\n",
    "In order to apply this idea, we choose some \"frames\" with shrinking size and some straight vertical lines moving along the grid and we perform the algorithm using them as patterns. Moreover we try our algorithm with random generated patterns with only $10$ black pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames\n",
    "frames = [np.zeros(N)-1 for j in range((L+1)//2)]\n",
    "\n",
    "for i in range((L+1)//2):\n",
    "    for j in range(N):\n",
    "        if j%L == i:\n",
    "            frames[i][j] = 1\n",
    "        if j%L == (L-i-1) :\n",
    "            frames[i][j] = 1\n",
    "        if j//L == i:\n",
    "            frames[i][j] = 1\n",
    "        if j//L == (L-i-1) :\n",
    "            frames[i][j] = 1\n",
    "    for k in range(i):\n",
    "        for j in range(N):\n",
    "            if j%L == k:\n",
    "                frames[i][j] = -1\n",
    "            if j%L == (L-k-1) :\n",
    "                frames[i][j] = -1\n",
    "            if j//L == k:\n",
    "                frames[i][j] = -1\n",
    "            if j//L == (L-k-1) :\n",
    "                frames[i][j] = -1\n",
    "\n",
    "fig, ax = plt.subplots(1, (L+1)//2 , figsize=(16,6))\n",
    "\n",
    "for i in range((L+1)//2):\n",
    "    ax[i].set_title('Pattern %i' %(i+1))\n",
    "    ax[i].imshow(frames[i].reshape(L,L),cmap='Greys')\n",
    "plt.show()    \n",
    "\n",
    "frames = np.array(frames)\n",
    "ind = np.arange(len(frames))\n",
    "\n",
    "images = [[],[]]\n",
    "\n",
    "for idx in ind: \n",
    "    for MF in range(2):\n",
    "        if MF: J = MF_coupling(frames,N)\n",
    "        else: J = R_coupling(frames,N,R)\n",
    "\n",
    "        sys = deepcopy(frames[idx])\n",
    "\n",
    "        sys = montecarlo(J, sys, 25, 500)\n",
    "        wrong_pixels = error_im(frames[idx],sys)\n",
    "        assigned_pattern = assign_pattern(frames,sys)\n",
    "        images[MF].append(sys)\n",
    "        \n",
    "#         print('The error of the algorithm using MF = %i is %f' %(MF, wrong_pixels))\n",
    "#         print('The algorithm recognised the pattern %i and the correct pattern is %i\\n' %(assigned_pattern,idx))\n",
    "\n",
    "fig, ax = plt.subplots(2, (L+1)//2 , figsize=(16,8))\n",
    "\n",
    "for i in range((L+1)//2):\n",
    "    ax[0][i].set_title('Pattern %i, Non Mean Field' %(i+1))\n",
    "    ax[0][i].imshow(images[0][i].reshape(L,L),cmap='Greys')\n",
    "    ax[1][i].set_title('Pattern %i, Mean Field' %(i+1))\n",
    "    ax[1][i].imshow(images[1][i].reshape(L,L),cmap='Greys')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight verical lines patterns\n",
    "\n",
    "patt = [np.zeros(N)-1 for j in range(L)]\n",
    "for i in range(L):\n",
    "    for j in range(L):\n",
    "        patt[i][i+L*j]=1\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, int(len(patt)/2), figsize=(22,10))\n",
    "\n",
    "for i in range(int(len(patt)/2)):\n",
    "    ax[0][i].set_title('Pattern %i' %(i+1))\n",
    "    ax[0][i].imshow(patt[i].reshape(L,L),cmap='Greys')\n",
    "    ax[1][i].set_title('Pattern %i' %(i+int(len(patt)/2)+1))\n",
    "    ax[1][i].imshow(patt[i+int(len(patt)/2)].reshape(L,L),cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# we try our algorithm to see if the patterns are distinguishable.\n",
    "\n",
    "patt = np.array(patt)\n",
    "ind = np.arange(len(patt))\n",
    "\n",
    "for idx in ind: \n",
    "    for MF in range(2):\n",
    "        if MF: J = MF_coupling(patt,N)\n",
    "        else: J = R_coupling(patt,N,R)\n",
    "\n",
    "        sys = deepcopy(patt[idx])\n",
    "\n",
    "        sys = montecarlo(J, sys, 25, 500)\n",
    "        wrong_pixels = error_im(patt[idx],sys)\n",
    "        assigned_pattern = assign_pattern(patt,sys)\n",
    "\n",
    "#         fig, ax = plt.subplots(1,2, figsize = (8,5))\n",
    "#         ax[0].set_title('Pattern')\n",
    "#         ax[0].imshow(patt[idx].reshape(L,L), cmap='Greys')\n",
    "#         ax[1].set_title('Finishing configuration')\n",
    "#         ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "#         plt.show()\n",
    "        print('The error of the algorithm using MF = %i is %f' %(MF, wrong_pixels))\n",
    "        print('The algorithm recognised the pattern %i and the correct pattern is %i\\n' %(assigned_pattern,idx))\n",
    "        \n",
    "fig, ax = plt.subplots(1,2, figsize = (8,5))\n",
    "ax[0].set_title('Pattern')\n",
    "ax[0].imshow(patt[-1].reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Finishing configuration')\n",
    "ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random\n",
    "diff = [np.zeros(N)-1 for j in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(L):\n",
    "        k = np.random.randint(0,N)\n",
    "        diff[i][k] = 1\n",
    "        \n",
    "\n",
    "diff = np.array(diff)\n",
    "ind = np.arange(len(diff))\n",
    "\n",
    "for idx in ind: \n",
    "    for MF in range(2):\n",
    "        if MF: J = MF_coupling(diff,N)\n",
    "        else: J = R_coupling(diff,N,R)\n",
    "\n",
    "        sys = deepcopy(diff[idx])\n",
    "\n",
    "        sys = deterministic_hopfield(sys,100)\n",
    "        wrong_pixels = error_im(diff[idx],sys)\n",
    "        assigned_pattern = assign_pattern(diff,sys)\n",
    "        if idx == 0 or idx == len(diff)-1:\n",
    "            fig, ax = plt.subplots(1,2, figsize = (8,5))\n",
    "            ax[0].set_title('Pattern')\n",
    "            ax[0].imshow(diff[idx].reshape(L,L), cmap='Greys')\n",
    "            ax[1].set_title('Finishing configuration')\n",
    "            ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "        plt.show()\n",
    "#         print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "#         print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these two examples we have seen that not all patterns are recognised:\n",
    "- the frames show a flaw in the recognition after a certain point;\n",
    "- the lines are never correctly classified: the system falls into the same minimum every time;\n",
    "- the random patterns also fall always in the same minimum, but it is different from the others.\n",
    "This behaviour is not impossible, as our system is frustrated: not all the spins can be in their optimal state and there is the possibility that stronger equilibria suppress the weaker ones.\n",
    "\n",
    "The first thing to do is to understand why that configuration is a minimum, since it is not one of the given patterns. \n",
    "\n",
    "Our guess is to look at the mean of the patterns, and indeed it proves to be the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize = (16,5))\n",
    "ax[0].set_title('Average over lines\\' patterns')\n",
    "ax[0].imshow(np.sign(patt.mean(axis=0)).reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Average over frames\\' patterns')\n",
    "ax[1].imshow(np.sign(frames.mean(axis=0)).reshape(L,L), cmap='Greys')\n",
    "ax[2].set_title('Average over random patterns')\n",
    "ax[2].imshow(np.sign(diff.mean(axis=0)).reshape(L,L), cmap='Greys')\n",
    "ax[3].set_title('Average over chessboards\\' patterns')\n",
    "ax[3].imshow(np.sign(chess.mean(axis=0)).reshape(L,L), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find a quantitative way to describe this similarity and to know if $p$ patterns are similar without running the algorithm. Since we are talking about a physical system, the proper parameter to define the similarity can be the energy. \n",
    "\n",
    "Our hypothesis is that if the average of the possible $p$ patterns has the lowest energy, then the set of considered $p$ patterns is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_fr = R_coupling(frames, N, R)\n",
    "J_patt = R_coupling(patt, N, R)\n",
    "J_diff = R_coupling(diff, N, R)\n",
    "J_chess = R_coupling(chess, N, R)\n",
    "\n",
    "energy_fr = []\n",
    "energy_patt = []\n",
    "energy_diff = []\n",
    "energy_chess = []\n",
    "\n",
    "for i in frames:\n",
    "    energy_fr.append( H(J_fr, i) )\n",
    "energy_fr.append(H(J_fr,np.sign(frames.mean(axis=0)) ) )\n",
    "\n",
    "for i in patt:\n",
    "    energy_patt.append( H(J_patt, i) )\n",
    "energy_patt.append(H(J_patt,np.sign(patt.mean(axis=0)) ) )\n",
    "\n",
    "for i in diff:\n",
    "    energy_diff.append( H(J_diff, i) )\n",
    "energy_diff.append(H(J_diff,np.sign(diff.mean(axis=0)) ) )\n",
    "\n",
    "for i in chess:\n",
    "    energy_chess.append( H(J_chess, i) )\n",
    "energy_chess.append(H(J_chess,np.sign(chess.mean(axis=0)) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize = (20,4))\n",
    "\n",
    "ax[0].set_title('Average over lines\\' patterns')\n",
    "ax[0].scatter(np.arange(6),energy_fr, marker='x', c='r')\n",
    "ax[0].set_ylabel(\"Energy\")\n",
    "ax[0].set_xlabel(\"Number of pattern\")\n",
    "\n",
    "ax[1].set_title('Average over frames\\' patterns')\n",
    "ax[1].scatter(np.arange(len(energy_patt)),energy_patt, marker='x', c='r')\n",
    "ax[1].set_xlabel(\"Number of pattern\")\n",
    "\n",
    "ax[2].set_title('Average over random patterns')\n",
    "ax[2].scatter(np.arange(len(energy_diff)),energy_diff, marker='x', c='r')\n",
    "ax[2].set_xlabel(\"Number of pattern\")\n",
    "\n",
    "ax[3].set_title('Average over chessboards\\' patterns')\n",
    "ax[3].scatter(np.arange(3),energy_chess, marker='x', c='r')\n",
    "ax[3].set_xlabel(\"Number of pattern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we proved that our guess was correct, and we found a criteria to predict whether a set of $p$ patterns is undistinguishable for the Hopfield model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand written digits\n",
    "\n",
    "Up to now, we have seen that our model recongize the simplest data that we generated. Now, we want to take a step further and try to implement the model in a more sophisticated dataset: the MINST. \n",
    "\n",
    "The MINST is a large database of handwritten digits of size 28x28 pixels that is commonly used for training various image processing. The aim of this section is to identify correctly the digits using the Hopfield Model. \n",
    "\n",
    "We will just use the test set to run our analysis since it is way smaller than the train one, but still have enough data to give meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert image to pixel\n",
    "#The value 0.2 was chosen arbitrary by considering the grey scale of the image. \n",
    "def from_minst_to_spin(X):\n",
    "    X = X/255 # Normalizing the values \n",
    "    X[ X<0.2 ] = -1  \n",
    "    X[ X>=0.2 ] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('./mnist_test.csv')\n",
    "data_train = pd.read_csv('./mnist_train.csv')\n",
    "\n",
    "L = 28\n",
    "N = L**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = []\n",
    "digits_train = []\n",
    "for i in range(data_train.shape[0]):\n",
    "    a = np.array( data_train.iloc[ [i] ])\n",
    "    label_train.append( a[0][0]) # Taking the digit\n",
    "    digits_train.append( a[0][1:] ) # Taking the image of the digit\n",
    "\n",
    "label_train = np.array(label_train)\n",
    "digits_train = np.array(digits_train)\n",
    "X_train = from_minst_to_spin(digits_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = []\n",
    "digits_test = []\n",
    "for i in range(data_test.shape[0]):\n",
    "    a = np.array( data_test.iloc[ [i] ])\n",
    "    label_test.append( a[0][0]) # Taking the digit\n",
    "    digits_test.append( a[0][1:] ) # Taking the image of the digit\n",
    "\n",
    "label_test = np.array(label_test)\n",
    "digits_test = np.array(digits_test)\n",
    "X_test = from_minst_to_spin(digits_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that needs to be done is to decide which are the possible patterns of the Hopfield model. It seems reasonable to take the average of the images representing the same digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with patterns that are the average over all the digits.\n",
    "# For example take the all 0, sum over the pixel and divide for the number of images.\n",
    "# If the value is negative -> -1, the other case 1\n",
    "mean_patterns = []\n",
    "for i in range(10):\n",
    "    mean_patterns.append( np.sign(X_train[ label_train == i ].sum(axis=0)) )\n",
    "mean_patterns = np.array(mean_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(20,8))\n",
    "\n",
    "for i in range(5):\n",
    "    ax[0][i].imshow( mean_patterns[i].reshape(L,L), cmap='Greys' )\n",
    "    ax[1][i].imshow( mean_patterns[i+5].reshape(L,L), cmap='Greys' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use MF Montecarlo\n",
    "idx = np.random.randint(0,9999)\n",
    "dgt = label_test[idx]\n",
    "\n",
    "J = MF_coupling(mean_patterns, N)\n",
    "sys = X_test[idx]\n",
    "\n",
    "res = montecarlo(J,sys,25,1000)\n",
    "\n",
    "wrong_pixels = error_im(mean_patterns[dgt],res)\n",
    "assigned_pattern = assign_pattern(mean_patterns,res)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(10,6))\n",
    "ax[0].set_title('Original pattern')\n",
    "ax[0].imshow(mean_patterns[dgt].reshape(L,L),cmap='Greys')\n",
    "ax[1].set_title('Handwritten digit')\n",
    "ax[1].imshow(sys.reshape(L,L),cmap='Greys')\n",
    "ax[2].set_title('Recovered digit')\n",
    "ax[2].imshow(res.reshape(L,L),cmap='Greys')\n",
    "ax[3].set_title('Recognised digit')\n",
    "ax[3].imshow(mean_patterns[assigned_pattern].reshape(L,L),cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,dgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen the recovered digit is strangely shaped, and multiple runs outline that the recognised digit is always the same: our set of patterns is undistinguishable for our algorithm with the previoulsy explained meaning. Hence, a suitable choice of the patterns must be done: firstly, we decide to \"zip\" the images in order to have less pixels to analyze.\n",
    "\n",
    "Another important reason to zip the image is the gaining in term of time implementation: we will at least divide that time by $4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipping the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zipping function takes batches of four pixels and from them creates a single pixel\n",
    "that is black even if only one of the four is black. This divides by 4 the resolution of the image, lowering the size of the system from 28x28 to 14x14.\n",
    "\n",
    "This is a common technique used in neural networks when several features are involved: indeed the choice of generating a black pixel, even if only one in four is black, is given by the different relevance that the two colors have in the MNIST problem. The white pixels are only background, and the blacks contain the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipping(sys1):\n",
    "    sys = deepcopy(sys1)\n",
    "    N = sys.shape[0]\n",
    "    L = int(np.sqrt(N))\n",
    "    sys = sys.reshape(L,L)\n",
    "    zipp = np.array([-1 for i in range(N//4)]).reshape(L//2,L//2)\n",
    "    for i in np.arange(0, L, 2):\n",
    "        for j in np.arange(0, L, 2):\n",
    "            if sys[i][j] + sys[i+1][j] + sys[i][j+1] + sys[i+1][j+1] > -4:\n",
    "                zipp[i//2,j//2] = 1\n",
    "    zipp = zipp.reshape(1,N//4)\n",
    "    return zipp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we zip all the data\n",
    "X_zip = []\n",
    "for i in X_test:\n",
    "    X_zip.append( zipping(i) )\n",
    "X_zip = np.array(X_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_zip = []\n",
    "\n",
    "for i in mean_patterns:\n",
    "    mean_zip.append( zipping(i) )\n",
    "mean_zip = np.array(mean_zip)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(20,8))\n",
    "\n",
    "for i in range(5):\n",
    "    ax[0][i].imshow( mean_zip[i].reshape(L//2,L//2), cmap='Greys' )\n",
    "    ax[1][i].imshow( mean_zip[i+5].reshape(L//2,L//2), cmap='Greys' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = R_coupling(mean_zip, N//4, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,9999)\n",
    "dgt = label_test[idx]\n",
    "\n",
    "sys = X_zip[idx]\n",
    "\n",
    "new = montecarlo(J,sys,25,1000)\n",
    "#res = deterministic_hopfield(sys, 100)\n",
    "\n",
    "wrong_pixels = error_im(mean_zip[dgt],new)\n",
    "assigned_pattern = assign_pattern(mean_zip,new)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(10,6))\n",
    "ax[0].set_title('Original pattern')\n",
    "ax[0].imshow(mean_zip[dgt].reshape(L//2,L//2),cmap='Greys')\n",
    "ax[1].set_title('Handwritten digit')\n",
    "ax[1].imshow(sys.reshape(L//2,L//2),cmap='Greys')\n",
    "ax[2].set_title('Recovered digit')\n",
    "ax[2].imshow(new.reshape(L//2,L//2),cmap='Greys')\n",
    "ax[3].set_title('Recognised digit')\n",
    "ax[3].imshow(mean_zip[assigned_pattern].reshape(L//2,L//2),cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,dgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if zipping the images reduces the implementation time, it does not solve the problem of the strange new minimum. Thus, we will see if the given patterns are stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the real minima of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find all the minima of the system we start from the mean patterns: from the theory we are sure that they should be the only minima of the system. Indeed we know that it is not totally true from the similarity analysis.\n",
    "\n",
    "We will so see if the given pattern are stable, and if not in which minimum they fall. We will also print the average of those patterns, to see if the theory presented above is still valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minima = []\n",
    "\n",
    "for k in mean_zip:\n",
    "    res = montecarlo(J, k, 25, 500)\n",
    "    minima.append(res)\n",
    "\n",
    "minima = np.array(minima)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(20,8))\n",
    "\n",
    "for i in range(5):\n",
    "    ax[0][i].set_title('digit %i' %i)\n",
    "    ax[0][i].imshow( minima[i].reshape(L//2,L//2), cmap='Greys' )\n",
    "    ax[1][i].set_title('digit %i' %(i+5))\n",
    "    ax[1][i].imshow( minima[i+5].reshape(L//2,L//2), cmap='Greys' )\n",
    "plt.show()\n",
    "\n",
    "energy_dig = []\n",
    "\n",
    "for i in mean_zip:\n",
    "    energy_dig.append( H(J, i) )\n",
    "energy_dig.append(H(J,np.sign(mean_zip.mean(axis=0)) ) )\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,6))\n",
    "ax[0].set_title('Average digit')\n",
    "ax[0].imshow( np.sign(minima.mean(axis=0)).reshape(L//2,L//2), cmap='Greys' )\n",
    "ax[1].set_title('Energy landscape of the digits')\n",
    "ax[1].scatter(np.arange(len(energy_dig)-1),energy_dig[:len(energy_dig)-1], marker='x', c='r', label= 'Digit energy')\n",
    "ax[1].scatter( len(energy_dig)-1, energy_dig[len(energy_dig)-1], marker='o', c='b', label= 'Average energy')\n",
    "ax[1].set_xlabel(\"Patterns\")\n",
    "ax[1].set_ylabel('Energy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can observe that all the minima found look almost the same: this means that the algorithm is unable to properly recognise the correct pattern, and we are in the very same situation discussed before. As a result, we develop a new method called \"Hopfield chain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopfield-chain algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model consist of several layers that are computed by observing the so called \"error matrix\". This matrix represent the rate of error in classifing the data comparing only two patterns at a time. In this way, we combine the most different patterns and make the algorithm choose between the most easily distinguishable ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_mat = []\n",
    "for i in range(10):\n",
    "    err_mat.append([])\n",
    "    for j in range(10):\n",
    "        err_mat[i].append( round(error_im(mean_zip[i], mean_zip[j]),2) )\n",
    "err_mat = np.array(err_mat).reshape(10,10)\n",
    "err_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting couples with higher differences\n",
    "best = []\n",
    "for i in range(10):\n",
    "    best.append( [i ,np.argmax( err_mat[i] )] )\n",
    "\n",
    "# Eliminating symmetric couples\n",
    "best = np.unique(np.sort(best, axis=1), axis=0)\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the couplings for the various connections\n",
    "Ji = []\n",
    "for i in best:\n",
    "    patt = np.array( [  mean_zip[i[0]], mean_zip[i[1]] ] )\n",
    "    Ji.append( R_coupling( patt ,N//4, 7) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average for each digits\n",
    "\n",
    "Now it is important to set a threshold error between layers. In order to derive it, we compute the speed of convergence for each digit as a function of the number of iterations. \n",
    "Finding the optimal value is crucial to avoid the incorrect classification of different patterns (i.e. all the patterns falling in the same minimum).\n",
    "We so first compute the speed of convergence of each digit in its layer, and then the speed of convergence of each digit in the first layer ( and so in a layer that doesn't have access to the correct classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at different digit convergence time\n",
    "ordered_digits = []\n",
    "for i in range(10):\n",
    "    ordered_digits.append( X_zip[ label_test == i ] )\n",
    "\n",
    "iterations = np.arange(200,600,50)\n",
    "\n",
    "err = []\n",
    "err_dev = []\n",
    "\n",
    "for i in range(10): # cycle over the digits\n",
    "    err.append( [] )\n",
    "    err_dev.append( [] )\n",
    "    for t in iterations: # cycle over iterations\n",
    "        temp_err = []\n",
    "        for n in range(10): # iterations\n",
    "            for j in range(len(best)): # Selection of correct coupling\n",
    "                if i in best[j]:\n",
    "                    sys = ordered_digits[i][n]\n",
    "                    res = montecarlo(Ji[j], sys ,25 , t)\n",
    "                    wrong_pixels = error_im(mean_zip[i], res)\n",
    "                    temp_err.append(wrong_pixels)\n",
    "                    break\n",
    "        temp_err = np.array(temp_err)\n",
    "        err[i].append( temp_err.mean() )\n",
    "        err_dev[i].append( temp_err.std() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize=(10,6) )\n",
    "for i in range(10):\n",
    "    label = ('Digit %i' %i)\n",
    "    plt.errorbar( iterations, err[i],fmt='--o', yerr=err_dev[i],  label = label )\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Errors')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Speed of convergence for each digit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\Images\\mnist_samedigit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convergence of all the digits can be observed, and it can be noticed that the maximum error is very low ($\\approx 7.5\\%$). By observing the graph, this value is set as threshold and the number of iterations is fixed to $400$.\n",
    "\n",
    "After that, we want to prove that the chosen values are correct, so we calculate the same plot but considering only the performances for every digit with respect to the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_digits = []\n",
    "\n",
    "for i in range(10):\n",
    "    diff_digits.append( X_zip[ label_test == i ] )\n",
    "\n",
    "iterations = np.arange(200,600,50)\n",
    "\n",
    "err = []\n",
    "err_dev = []\n",
    "\n",
    "for i in range(2,10): # cycle over the digits\n",
    "    err.append( [] )\n",
    "    err_dev.append( [] )\n",
    "    for t in iterations: # cycle over iterations\n",
    "        temp_err = []\n",
    "        for n in range(10): # iterations\n",
    "            sys = ordered_digits[i][n]\n",
    "            res = montecarlo(Ji[0], sys ,25 , t)\n",
    "            wrong_pixels = error_im(mean_zip[i], res)\n",
    "            temp_err.append(wrong_pixels)\n",
    "        temp_err = np.array(temp_err)\n",
    "        err[i-2].append( temp_err.mean() )\n",
    "        err_dev[i-2].append( temp_err.std() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize=(10,6) )\n",
    "for i in range(2, 10):\n",
    "    label = ('Digit %i' %i)\n",
    "    plt.errorbar( iterations, err[i-2],fmt='--o', yerr=err_dev[i-2],  label = label )\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Errors')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Speed of convergence for each digit with respect to layer 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='.\\Images\\minst_diffdigit.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that every curve in the graph above is clearly over the chosen threshold of $0.075$ and that a divergence can be noticed, which is the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "score = 0\n",
    "\n",
    "for n in range(100):\n",
    "    idx = np.random.randint(0,9999)\n",
    "    dgt = label_test[idx]\n",
    "\n",
    "    for i in range(len(best)):\n",
    "        sys = X_zip[idx]\n",
    "        res = montecarlo(Ji[i], sys ,25 , 400)\n",
    "\n",
    "        wrong_pixels = error_im(mean_zip[dgt], res)\n",
    "\n",
    "        if wrong_pixels > 0.075 and i<8: \n",
    "            continue\n",
    "        patt = np.array( [  mean_zip[best[i][0]], mean_zip[best[i][1]] ] )\n",
    "        assigned_pattern = assign_pattern(patt, res) \n",
    "        assigned_pattern = best[i][assigned_pattern] \n",
    "        break\n",
    "    err.append(wrong_pixels)\n",
    "    if assigned_pattern == dgt: score += 1\n",
    "\n",
    "err = np.array(err)\n",
    "score /= 100\n",
    "print('The score of the algorithm is %f with an average error of %f' %(score, err.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "score = 0\n",
    "for n in range(len(X_zip)):\n",
    "    idx = n\n",
    "    dgt = label_test[idx]\n",
    "\n",
    "    for i in range(len(best)):\n",
    "        sys = X_zip[idx]\n",
    "        res = montecarlo(Ji[i], sys ,25 , 400)\n",
    "\n",
    "        wrong_pixels = error_im(mean_zip[dgt], res)\n",
    "\n",
    "        if wrong_pixels > 0.10 and i<8: \n",
    "            continue\n",
    "        patt = np.array( [  mean_zip[best[i][0]], mean_zip[best[i][1]] ] )\n",
    "        assigned_pattern = assign_pattern(patt, res) \n",
    "        assigned_pattern = best[i][assigned_pattern] \n",
    "        break\n",
    "    err.append(wrong_pixels)\n",
    "    if assigned_pattern == dgt: score += 1\n",
    "\n",
    "err = np.array(err)\n",
    "score /= len(X_zip)\n",
    "print('The score of the algorithm is %f with an average error of %f' %(score, err.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion (by Marco)\n",
    "In this project we underline the capabilities and the flaws of the Hopfield model, by optimizing its parameters. \n",
    "As we expected, a similarity of the time implementation using MF and NMF was observed. On the other hand, we outlined the less fluctuations given by the MF. Moreover, we analyzed the influence of different types of noises on the performances of the algorithm in order to get more accurate results.\n",
    "\n",
    "We concluded that the Montecarlo simulation leads to better results than the deterministic update rule, as well as a greater efficency given by a much faster implementation time.\n",
    "\n",
    "In addition, we saw that the mapping between the neural network and the physical system is well defined. According to this, we managed to define the similarity between two patterns by using a physical quantity: the energy.\n",
    "\n",
    "After a long detailed analysis of the basic theory behind the model, we finally tried it on a real world task: the recognition of the MNIST digits. The simulation was adapted by considering sets of layers corresponding to different coupled patterns. The algorithm gives indeed amazing results after some trials: we managed to classify correctly the $90\\%$ of the digits, with an average error of $0.02\\pm 0.03$, really close and compatible to zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- stability: \n",
    "- study of the dependence of error of the algorithm with respect to number of patterns; \n",
    "- time scaling with size of the system;\n",
    "- time scaling with the number of patterns;\n",
    "- dependence of the error on the number of neighbors. \n",
    "\n",
    "- corruption: different noises observed, correctly recovered both MF and NMF, tendency of parameters studied\n",
    "\n",
    "- montecarlo: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    " \n",
    "MoTP notes by Marco Baeisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HopfieldModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
